{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zahrahimi/ReluplexCav2017/blob/master/bigram_sparse_multiplicative_update.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35988526",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35988526",
        "outputId": "0394f578-e165-4b93-b66e-5d5e5747377c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8ae6972",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8ae6972",
        "outputId": "ca6cef99-26cf-4057-9c7e-12cfd9bba7c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num_ngram =  3\n",
            "920154\n",
            "vocab_size: 9931\n",
            "train_data_all: ['aer', 'banknote', 'berlitz', 'calloway', 'centrust', 'cluett', 'fromstein', 'gitano', 'guterman', 'hydroquebec']\n",
            "test_trigrams: [('no', 'it', 'was'), ('it', 'was', 'nt'), ('was', 'nt', 'black'), ('nt', 'black', 'monday'), ('black', 'monday', '<eos>'), ('monday', '<eos>', 'but'), ('<eos>', 'but', 'while'), ('but', 'while', 'the'), ('while', 'the', 'new'), ('the', 'new', 'york')]\n",
            "test_size: 81673\n",
            "test_bigrams: [('no', 'it'), ('it', 'was'), ('was', 'nt'), ('nt', 'black'), ('black', 'monday'), ('monday', '<eos>'), ('<eos>', 'but'), ('but', 'while'), ('while', 'the'), ('the', 'new')]\n",
            "test_size2: 81674\n"
          ]
        }
      ],
      "source": [
        "# Train and Test Dataset\n",
        "\n",
        "import nltk\n",
        "from nltk import word_tokenize, ngrams\n",
        "nltk.download('punkt')\n",
        "import re\n",
        "import numpy as np\n",
        "import random\n",
        "import time\n",
        "import math\n",
        "from itertools import permutations\n",
        "# import cyipopt\n",
        "\n",
        "\n",
        "num_ngram = 3\n",
        "print(\"num_ngram = \", num_ngram)\n",
        "\n",
        "# Specify the file paths\n",
        "train_file = 'ptb.train.txt'\n",
        "valid_file = 'ptb.valid.txt'\n",
        "test_file = 'ptb.test.txt'\n",
        "\n",
        "\n",
        "# Read the data from the files\n",
        "def read_data(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        data = file.read()\n",
        "        # Remove special characters and extra spaces\n",
        "        data = re.sub(r\"[^a-zA-Z0-9\\s<unk>]\", \"\", data)  ############remove any character that is not a letter, digit, whitespace, <unk>\n",
        "        # Convert to lowercase\n",
        "        data = data.lower().strip()\n",
        "        # Split into sentences and strip leading/trailing spaces from each sentence\n",
        "        data = [sentence.strip().split() for sentence in data.split('\\n')]\n",
        "        # data = [item for sublist in data for item in  sublist]\n",
        "        data = [item for sublist in data for item in sublist +['<eos>']]\n",
        "        # data = [item for sublist in data for item in sublist]\n",
        "\n",
        "    return data\n",
        "\n",
        "# Read the training data\n",
        "train_data_all = read_data(train_file)\n",
        "print(len(train_data_all))\n",
        "vocab = sorted(list(set(train_data_all)))\n",
        "vocab_size = len(vocab)\n",
        "print('vocab_size:',vocab_size)\n",
        "print('train_data_all:', train_data_all[:10])\n",
        "# Read the validation data\n",
        "valid_data_all = read_data(valid_file)\n",
        "\n",
        "# Read the test data\n",
        "test_data_all = read_data(test_file)\n",
        "\n",
        "#--------------------------------------------trigram\n",
        "\n",
        "def generate_trigrams(data):\n",
        "    # Create trigrams by zipping over the data offset by one word each time\n",
        "    return list(zip(data, data[1:], data[2:]))\n",
        "\n",
        "test_trigrams = generate_trigrams(test_data_all)\n",
        "test_size = len(test_trigrams)\n",
        "print('test_trigrams:', test_trigrams[:10])\n",
        "print('test_size:', test_size)\n",
        "\n",
        "\n",
        "train_trigrams = generate_trigrams(train_data_all)\n",
        "train_size = len(train_trigrams)\n",
        "\n",
        "#--------------------------------------------bigram\n",
        "def generate_bigrams(data):\n",
        "    # Create trigrams by zipping over the data offset by one word each time\n",
        "    return list(zip(data, data[1:]))\n",
        "\n",
        "\n",
        "test_bigrams = generate_bigrams(test_data_all)\n",
        "test_size2 = len(test_bigrams)\n",
        "print('test_bigrams:', test_bigrams[:10])\n",
        "print('test_size2:', test_size2)\n",
        "\n",
        "\n",
        "train_bigrams = generate_bigrams(train_data_all)\n",
        "train_size2 = len(train_bigrams)\n",
        "\n",
        "\n",
        "# def split_data(tokens, n):\n",
        "#     # Tokenize the first document into words\n",
        "#     # Generate n-grams from the list of tokens\n",
        "#     # Directly iterate over the generator\n",
        "#     lst_ngrams = list(ngrams(tokens, n))\n",
        "#     # Use list comprehension for efficiency\n",
        "#     lst_xs = [ngram[:-1] for ngram in lst_ngrams]\n",
        "#     lst_y = [ngram[-1] for ngram in lst_ngrams]\n",
        "\n",
        "#     return lst_xs, lst_y\n",
        "\n",
        "#--------------------------------------------my example\n",
        "# def split_data(tokens, n):\n",
        "#     # Tokenize the first document into words\n",
        "#     tokens = word_tokenize(tokens[0])\n",
        "#     # print('tokens', tokens)\n",
        "\n",
        "#     # Generate n-grams from the list of tokens\n",
        "#     # Directly iterate over the generator\n",
        "#     lst_ngrams = list(ngrams(tokens, n))\n",
        "#     # Use list comprehension for efficiency\n",
        "#     lst_xs = [ngram[:-1] for ngram in lst_ngrams]\n",
        "#     lst_y = [ngram[-1] for ngram in lst_ngrams]\n",
        "\n",
        "#     return lst_xs, lst_y\n",
        "\n",
        "# Example usage:\n",
        "# train_data_all = [\"he likes to play tennis\"]\n",
        "# train_data_all = [\"he likes to\"]\n",
        "# vocab = sorted(list(set(word_tokenize(train_data_all[0]))))\n",
        "# print('vocab =', vocab)\n",
        "# vocab_size = len(vocab)\n",
        "# print('vocab_size= ', vocab_size)\n",
        "# print('train_data_all =', train_data_all[0])\n",
        "#--------------------------------------------\n",
        "\n",
        "# lst_xs, lst_y = split_data(train_data_all, num_ngram)\n",
        "# print(\"lst_xs=\", lst_xs)\n",
        "# print(\"lst_y=\", lst_y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "36d1dda3",
        "outputId": "36a47d4e-ef07-45e3-ac75-c8376fc77d33"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "C_train_csr_dense:\n",
            " [[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n"
          ]
        }
      ],
      "source": [
        "# Creating Sparse trigram C_train and C_test\n",
        "\n",
        "from scipy.sparse import coo_array\n",
        "import numpy as np\n",
        "\n",
        "def CreateSparseTensor(data, word_to_index, vocab_size, slice_='True'):\n",
        "    # Lists to accumulate the non-zero values and their coordinates\n",
        "    values = []  # Non-zero data entries\n",
        "    row_indices = []  # Row coordinates\n",
        "    col_indices = []  # Column coordinates (flattened last two dimensions)\n",
        "    data_size = 0\n",
        "\n",
        "    # Process the input data to populate 'values' and 'coords'\n",
        "    for i in range(len(data) - 2):\n",
        "        word1, word2, word3 = data[i], data[i + 1], data[i + 2]\n",
        "        if word1 in word_to_index and word2 in word_to_index and word3 in word_to_index:\n",
        "            i2 = word_to_index[word1]\n",
        "            i1 = word_to_index[word2]\n",
        "            j = word_to_index[word3]\n",
        "            data_size += 1\n",
        "\n",
        "            # Append the data value (1 for each trigram occurrence)\n",
        "            values.append(1)\n",
        "\n",
        "            if slice_ == 'True':\n",
        "              # Append the row and column indices\n",
        "              row_indices.append(i2)\n",
        "              col_indices.append(i1 * vocab_size + j)  # Flatten the 2D indices into 1D\n",
        "\n",
        "            else:\n",
        "              row_indices.append(i1)\n",
        "              col_indices.append(i2 * vocab_size + j)  # Flatten the 2D indices into 1D\n",
        "\n",
        "    # Convert lists to numpy arrays\n",
        "    data_np = np.array(values)\n",
        "    row_np = np.array(row_indices)\n",
        "    col_np = np.array(col_indices)\n",
        "\n",
        "    # Create the sparse COO matrix\n",
        "    tensor_coo = coo_array((data_np, (row_np, col_np)), shape=(vocab_size, vocab_size * vocab_size), dtype=np.float32)\n",
        "    tensor_csr = tensor_coo.tocsr()         # Removing Duplicates by Converting Formats\n",
        "    # # Note: Create the sparse CSR matrix, Using vocab_size**2 for the number of columns as the last two dimensions are flattened\n",
        "    # C_csr = csr_matrix((values, (rows, cols)), shape=(vocab_size, vocab_size**2))\n",
        "\n",
        "    return tensor_csr\n",
        "\n",
        "\n",
        "#-------------------------------------\n",
        "# Example usage:\n",
        "# vocab = [\"0\", \"1\", \"2\"]\n",
        "# train_data_all = [\"0\", \"1\", \"2\", \"0\", \"1\", \"2\",\"1\",\"0\",\"1\"]\n",
        "# test_data_all = [\"0\", \"0\", \"2\", \"0\"]\n",
        "# def generate_trigrams(data):\n",
        "#     # Create trigrams by zipping over the data offset by one word each time\n",
        "#     return list(zip(data, data[1:], data[2:]))\n",
        "\n",
        "# test_trigrams = generate_trigrams(test_data_all)\n",
        "# test_size = len(test_trigrams)\n",
        "#-------------------------------------\n",
        "\n",
        "\n",
        "word_to_index = {word: index for index, word in enumerate(vocab)}\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "C_train_csr = CreateSparseTensor(train_data_all, word_to_index, vocab_size,'True')\n",
        "print('C_train_csr_dense:\\n', C_train_csr.todense())\n",
        "\n",
        "C_train_csr_slice2 = CreateSparseTensor(train_data_all, word_to_index, vocab_size,'False')\n",
        "# print('C_train_csr_dense_slice2:\\n', C_train_csr_slice2.todense())\n",
        "\n",
        "C_test = CreateSparseTensor(test_data_all, word_to_index, vocab_size,'True')\n",
        "# print('C_test:\\n', C_test.todense())\n",
        "\n",
        "# print('test_size:',test_size)\n",
        "\n",
        "# print('C_train_csr\\n:', C_train_csr)\n",
        "# print('C_train_csr_dense:\\n', C_train_csr.todense())\n",
        "# print('sparse.nnz:\\n', C_train_csr.nnz)\n",
        "# print('sum on rows\\n:', C_train_csr.sum(axis=0))\n",
        "# print('sum on columns\\n:', C_train_csr.sum(axis=1))\n",
        "# print('data\\n:', C_train_csr.data)\n",
        "# # print('slicing_dense:\\n', C_train_csr.todense()[[1], :])\n",
        "# # print('slicing_sparse:\\n', C_train_csr.tocsr()[[1], :])\n",
        "# # print('transpose_dense:\\n', C_train_csr.T.todense())\n",
        "# # print('transpose_sparse:\\n', C_train_csr.T)\n",
        "# # print('remove zeros:\\n', C_train_csr.tocsr().eliminate_zeros())\n",
        "# print('p1\\n:', C_train_csr.sum(axis=0))\n",
        "# print(type(C_train_csr))\n",
        "\n",
        "# lambda_matrix = np.array(\n",
        "#     [[1, 0, 0.3],  # Flattened representation of first \"row\" in 3D space\n",
        "#     [0, 5, 4],  # Flattened representation of second \"row\" in 3D space\n",
        "#     [0.1, 3, 0]]  # Flattened representation of third \"row\" in 3D space\n",
        "#  , dtype = np.float32)\n",
        "# lambda_matrix = csr_matrix(lambda_matrix)\n",
        "# print('lambda_matrix:\\n',lambda_matrix.todense())\n"
      ],
      "id": "36d1dda3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d3d0f2d-e958-4005-ac43-46d3b4aa6a69",
        "id": "5PDT5Xwlp-5Z"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CP1:\n",
            " [[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "CP2:\n",
            " [[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n"
          ]
        }
      ],
      "source": [
        "# Creating Sparse bigram C_train and C_test\n",
        "\n",
        "from scipy.sparse import coo_array\n",
        "import numpy as np\n",
        "\n",
        "def CreateCP(data, word_to_index, vocab_size, slice_):\n",
        "    # Lists to accumulate the non-zero values and their coordinates\n",
        "    values = []  # Non-zero data entries\n",
        "    row_indices = []  # Row coordinates\n",
        "    col_indices = []  # Column coordinates (flattened last two dimensions)\n",
        "\n",
        "    # Process the input data to populate 'values' and 'coords'\n",
        "    for i in range(len(data) - 2):\n",
        "        word1, word2, word3 = data[i], data[i + 1], data[i + 2]\n",
        "        if word1 in word_to_index and word2 in word_to_index and word3 in word_to_index:\n",
        "            i2 = word_to_index[word1]\n",
        "            i1 = word_to_index[word2]\n",
        "            j = word_to_index[word3]\n",
        "\n",
        "            # Append the data value (1 for each trigram occurrence)\n",
        "            values.append(1)\n",
        "\n",
        "            if slice_ == 'P1':\n",
        "              # Append the row and column indices\n",
        "              row_indices.append(i1)\n",
        "              col_indices.append(j)\n",
        "\n",
        "            elif slice_ == 'P2':\n",
        "              row_indices.append(i2)\n",
        "              col_indices.append(j)  # Flatten the 2D indices into 1D\n",
        "\n",
        "    # Convert lists to numpy arrays\n",
        "    data_np = np.array(values)\n",
        "    row_np = np.array(row_indices)\n",
        "    col_np = np.array(col_indices)\n",
        "\n",
        "    # Create the sparse COO matrix\n",
        "    tensor_coo = coo_array((data_np, (row_np, col_np)), shape=(vocab_size, vocab_size), dtype=np.float32)\n",
        "    tensor_csr = tensor_coo.tocsr()         # Removing Duplicates by Converting Formats\n",
        "    # # Note: Create the sparse CSR matrix, Using vocab_size**2 for the number of columns as the last two dimensions are flattened\n",
        "    # C_csr = csr_matrix((values, (rows, cols)), shape=(vocab_size, vocab_size**2))\n",
        "\n",
        "    return tensor_csr\n",
        "\n",
        "#-------------------------------------\n",
        "# Example usage:\n",
        "# vocab = [\"0\", \"1\", \"2\"]\n",
        "# train_data_all = [\"0\", \"1\", \"2\", \"0\", \"1\", \"2\",\"1\",\"0\",\"1\"]\n",
        "# test_data_all = [\"0\", \"0\", \"2\", \"0\", \"2\"]\n",
        "# def generate_trigrams(data):\n",
        "#     # Create trigrams by zipping over the data offset by one word each time\n",
        "#     return list(zip(data, data[1:], data[2:]))\n",
        "\n",
        "# test_trigrams = generate_trigrams(test_data_all)\n",
        "# test_size = len(test_trigrams)\n",
        "\n",
        "\n",
        "# def generate_bigrams(data):\n",
        "#     # Create bigrams by zipping over the data offset by one word each time\n",
        "#     return list(zip(data, data[1:]))\n",
        "\n",
        "# test_bigrams = generate_bigrams(test_data_all)\n",
        "# test_size = len(test_bigrams)\n",
        "#-------------------------------------\n",
        "\n",
        "\n",
        "word_to_index = {word: index for index, word in enumerate(vocab)}\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "CP1 = CreateCP(train_data_all, word_to_index, vocab_size,'P1')\n",
        "print('CP1:\\n', CP1.todense())\n",
        "\n",
        "\n",
        "CP2 = CreateCP(train_data_all, word_to_index, vocab_size,'P2')\n",
        "print('CP2:\\n', CP2.todense())\n",
        "\n",
        "# C_train_csr_slice2 = CreateSparseTensor(train_data_all, word_to_index, vocab_size,'False')\n",
        "# print('C_train_csr_dense_slice2:\\n', C_train_csr_slice2.todense())\n",
        "\n",
        "# CP1_test = CreateCP1(test_data_all, word_to_index, vocab_size,'True')\n",
        "# print('CP1_test:\\n', CP1_test.todense())\n",
        "\n",
        "# print('test_size:',test_size)\n",
        "\n",
        "# print('C_train_csr\\n:', C_train_csr)\n",
        "# print('C_train_csr_dense:\\n', C_train_csr.todense())\n",
        "# print('sparse.nnz:\\n', C_train_csr.nnz)\n",
        "# print('sum on rows\\n:', C_train_csr.sum(axis=0))\n",
        "# print('sum on columns\\n:', C_train_csr.sum(axis=1))\n",
        "# print('data\\n:', C_train_csr.data)\n",
        "# # print('slicing_dense:\\n', C_train_csr.todense()[[1], :])\n",
        "# # print('slicing_sparse:\\n', C_train_csr.tocsr()[[1], :])\n",
        "# # print('transpose_dense:\\n', C_train_csr.T.todense())\n",
        "# # print('transpose_sparse:\\n', C_train_csr.T)\n",
        "# # print('remove zeros:\\n', C_train_csr.tocsr().eliminate_zeros())\n",
        "# print('p1\\n:', C_train_csr.sum(axis=0))\n",
        "# print(type(C_train_csr))\n",
        "\n",
        "# lambda_matrix = np.array(\n",
        "#     [[1, 0, 0.3],  # Flattened representation of first \"row\" in 3D space\n",
        "#     [0, 5, 4],  # Flattened representation of second \"row\" in 3D space\n",
        "#     [0.1, 3, 0]]  # Flattened representation of third \"row\" in 3D space\n",
        "#  , dtype = np.float32)\n",
        "# lambda_matrix = csr_matrix(lambda_matrix)\n",
        "# print('lambda_matrix:\\n',lambda_matrix.todense())\n"
      ],
      "id": "5PDT5Xwlp-5Z"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f557c012",
      "metadata": {
        "id": "f557c012"
      },
      "outputs": [],
      "source": [
        "from scipy.sparse import csr_matrix\n",
        "import numpy as np\n",
        "\n",
        "def DistinctTokenSparse(CP):\n",
        "    \"\"\"\n",
        "    Computes a distinct token count for each row in a sparse matrix.\n",
        "\n",
        "    Parameters:\n",
        "    - CP: A sparse CSR matrix.\n",
        "\n",
        "    Returns:\n",
        "    - distinct_tokens: A NumPy array where each element is the sum of elements less than 1\n",
        "                       and the count of elements greater than 1 for each row in CP.\n",
        "    \"\"\"\n",
        "    # Convert sparse matrix to COO format for element-wise operations\n",
        "    CP_coo = CP.tocoo()\n",
        "\n",
        "    # Find elements less than 1 and create a sparse matrix of the same shape with these elements\n",
        "    elements_less_than_one = csr_matrix((np.where(CP_coo.data < 1, CP_coo.data, 0), (CP_coo.row, CP_coo.col)), shape=CP.shape)\n",
        "\n",
        "    # Sum these modified values row-wise\n",
        "    row_sums_less_than_one = np.array(elements_less_than_one.sum(axis=1)).squeeze()\n",
        "\n",
        "    # For elements greater than 1, we create a binary sparse matrix and sum row-wise\n",
        "    elements_greater_than_one = csr_matrix((np.where(CP_coo.data >= 1, 1, 0), (CP_coo.row, CP_coo.col)), shape=CP.shape)\n",
        "    count_greater_than_one_per_row = np.array(elements_greater_than_one.sum(axis=1)).squeeze()\n",
        "\n",
        "    # Combine the two metrics\n",
        "    distinct_tokens = row_sums_less_than_one + count_greater_than_one_per_row\n",
        "\n",
        "    return distinct_tokens.reshape(-1, 1)\n",
        "\n",
        "# # Example usage\n",
        "# # P1_normalized_sparse = csr_matrix(np.array([[0.5, 1, 0.3], [1.5, 0.2, 0.0], [0.0, 0.0, 3.0]]))\n",
        "# print('P1_normalized_sparse:\\n', P1_normalized_sparse.todense())\n",
        "# # Assuming P1_normalized_sparse is your input sparse matrix in CSR format\n",
        "\n",
        "# P1_normalized_sparse = CP1\n",
        "# distinct_tokens_sparse = DistinctTokenSparse(P1_normalized_sparse)\n",
        "# print('Distinct tokens (sparse handling):\\n', distinct_tokens_sparse)\n",
        "\n",
        "# print('P2_normalized_sparse:\\n', P2_normalized_sparse.todense())\n",
        "# distinct_tokens_sparse2 = DistinctTokenSparse(P2_normalized_sparse)\n",
        "# print('Distinct tokens2 (sparse handling):\\n', distinct_tokens_sparse2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a7398fb",
      "metadata": {
        "id": "0a7398fb"
      },
      "outputs": [],
      "source": [
        "from scipy.sparse import csr_matrix\n",
        "import numpy as np\n",
        "\n",
        "def Smoothing_P_sparse(CP_slice, alpha):\n",
        "    \"\"\"\n",
        "    Efficiently process a single slice of CP_slice in sparse CSR format to create\n",
        "    a corresponding slice of A using sparse operations, ensuring correct division by row sums.\n",
        "\n",
        "    Parameters:\n",
        "    - CP_slice: A sparse CSR matrix slice representing part of the data.\n",
        "    - alpha: Smoothing parameter.\n",
        "\n",
        "    Returns:\n",
        "    - A_slice: A sparse CSR matrix after applying the smoothing operation.\n",
        "    - n_i: Sum of elements in each row, adjusted to avoid division by zero.\n",
        "    \"\"\"\n",
        "    # Ensure CP_slice is in CSR format\n",
        "    if not isinstance(CP_slice, csr_matrix):\n",
        "        CP_slice = csr_matrix(CP_slice)\n",
        "\n",
        "    # Compute row sums (n_i) with a small value added to avoid division by zero\n",
        "    # n_i = CP_slice.sum(axis=1).A1 + 1e-10\n",
        "    n_i = CP_slice.sum(axis=1).A1\n",
        "    n_i[n_i == 0] = 1\n",
        "\n",
        "    # Compute row sums for normalization\n",
        "    # row_sums = P1_updated.sum(axis=1)\n",
        "    # row_sums[row_sums == 0] = 1  # Avoid division by zero\n",
        "    # # Compute row sums for normalization\n",
        "    # row_sums = np.sum(P1_reshaped, axis=1, keepdims=True)\n",
        "    # row_sums[row_sums == 0] = 1  # Avoid division by zero\n",
        "\n",
        "\n",
        "    # Create masks for elements >= 1 and < 1\n",
        "    greater_mask = CP_slice.data >= 1\n",
        "    print('CP_slice:', CP_slice.todense())\n",
        "    less_mask = CP_slice.data < 1\n",
        "\n",
        "    # Initialize A_slice as a copy of CP_slice for output\n",
        "    A_slice = CP_slice.copy()\n",
        "\n",
        "    # Get the row index for each non-zero element in CP_slice\n",
        "    row_indices = np.repeat(np.arange(CP_slice.shape[0]), np.diff(CP_slice.indptr))\n",
        "    # print('CP_slice.indptr', CP_slice.indptr)\n",
        "    # print('row_indices:/n',row_indices)\n",
        "    # print('row_indices[greater_mask]:/n',row_indices[greater_mask])\n",
        "    # print('n_i[row_indices[greater_mask]]:/n',n_i[row_indices[greater_mask]])\n",
        "    # print('A_slice.data[greater_mask]', A_slice.data[greater_mask])\n",
        "    # print('greater_mask:', greater_mask, 'CP_slice.indptr:', CP_slice.indptr, 'np.diff(CP_slice.indptr):', np.diff(CP_slice.indptr),'row_indices[greater_mask]', row_indices[greater_mask], n_i[row_indices[greater_mask]])\n",
        "\n",
        "    # Apply smoothing operation directly on the data of the sparse matrix\n",
        "    A_slice.data[greater_mask] = (A_slice.data[greater_mask] - alpha) / n_i[row_indices[greater_mask]]\n",
        "    A_slice.data[less_mask] = (A_slice.data[less_mask] * (1 - alpha)) / n_i[row_indices[less_mask]]\n",
        "\n",
        "    return A_slice, n_i.reshape(-1, 1)\n",
        "\n",
        "# Usage example:\n",
        "# CP_slice should be a sparse CSR matrix and alpha a float\n",
        "# Example usage assuming P1_normalized_sparse is a sparse CSR matrix\n",
        "# P1_normalized_sparse = csr_matrix(np.array([[1.5, 2.0, 0.3], [1.5, 0.2, 0.0], [0.0, 0.0, 0.0]]))\n",
        "# print('P1_normalized_sparse:\\n',P1_normalized_sparse)\n",
        "# print('P1_normalized_dense:\\n',P1_normalized_sparse.todense())\n",
        "# P1_normalized_sparse = CP1_test\n",
        "# smoothed_P, n_i = Smoothing_P_sparse(P1_normalized_sparse, alpha=0.8)\n",
        "# print('smoothed_P:\\n', smoothed_P.todense())\n",
        "# print('n_i:\\n', n_i)\n",
        "\n",
        "# print('P2_normalized_dense:\\n',P2_normalized_sparse.todense())\n",
        "# smoothed_P2, n_i2 = Smoothing_P_sparse(P2_normalized_sparse, alpha=0.8)\n",
        "# print('smoothed_P2:\\n', smoothed_P2.todense())\n",
        "# print('n_i2:\\n', n_i2)\n",
        "\n",
        "# CP_slice: [[0. 3. 0.]\n",
        "#  [1. 0. 2.]\n",
        "#  [1. 1. 0.]]\n",
        "# greater_mask: [ True  True  True  True  True] CP_slice.indptr: [0 1 3 5] np.diff(CP_slice.indptr): [1 2 2] row_indices[greater_mask] [0 1 1 2 2] [3. 3. 3. 2. 2.]\n",
        "# smoothed_P:\n",
        "#  [[0.         1.         0.        ]\n",
        "#  [0.33333334 0.         0.6666667 ]\n",
        "#  [0.5        0.5        0.        ]]\n",
        "# n_i:\n",
        "#  [[3.]\n",
        "#  [3.]\n",
        "#  [2.]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RCg4tezVpFVz",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RCg4tezVpFVz",
        "outputId": "6259a9e6-cb35-4f21-e12b-835146dc86f6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "# Correct one\n",
        "import numpy as np\n",
        "import gc\n",
        "import scipy.sparse as sp\n",
        "from scipy.sparse import csr_matrix, find\n",
        "\n",
        "\n",
        "# # Assuming CT, P1, P2, and lambda_matrix are already in CSR format\n",
        "# CT = C_train_csr_slice2\n",
        "# print('CT:\\n', CT.todense())\n",
        "# # Set seed for reproducibility\n",
        "# np.random.seed(42)\n",
        "# lambda_matrix = np.random.rand(vocab_size, vocab_size)\n",
        "# lambda_matrix = csr_matrix(lambda_matrix)\n",
        "# P1 = P1_normalized_sparse\n",
        "# P2 = P2_normalized_sparse\n",
        "\n",
        "def get_original_indices(flat_idx, vocab_size):\n",
        "    \"\"\"Convert flat index back to (i2, j) index pair\"\"\"\n",
        "    i2 = flat_idx // vocab_size\n",
        "    j = flat_idx % vocab_size\n",
        "    return i2, j\n",
        "\n",
        "\n",
        "def update_p1_sparse(CT, P1_csr, P2_csr, lambda_csr, vocab_size):\n",
        "    # Ensure all matrices are in CSR format for efficient row access and arithmetic operations\n",
        "    # CT_csr = CT.tocsr()\n",
        "    # P1_csr = P1.tocsr()\n",
        "    # P2_csr = P2.tocsr()\n",
        "    # lambda_csr = lambda_matrix.tocsr()\n",
        "\n",
        "\n",
        "    # Initialize P1_updated as a LIL matrix for efficient row-wise operations\n",
        "    P1_updated = sp.lil_matrix((vocab_size, vocab_size))\n",
        "\n",
        "\n",
        "    # Iterate through non-zero elements in CT\n",
        "    CT_nonzero = find(CT)\n",
        "    for i, idx, c_val in zip(*CT_nonzero):\n",
        "        i1 = i\n",
        "        i2, j = get_original_indices(idx, vocab_size)\n",
        "\n",
        "        # Directly extract values from CSR matrices\n",
        "        p1_val = P1_csr[i1, j] if P1_csr[i1, j] != 0 else 0\n",
        "        p2_val = P2_csr[i2, j] if P2_csr[i2, j] != 0 else 0\n",
        "        lambda_val = lambda_csr[i1, i2] if lambda_csr[i1, i2] != 0 else 0\n",
        "\n",
        "        # Compute the update for P1 based on the formula\n",
        "        denominator1 = lambda_val * p1_val + (1 - lambda_val) * p2_val\n",
        "        # denominator[denominator == 0] = 1\n",
        "        denominator = denominator1 if denominator1 != 0 else 1\n",
        "        # if denominator != 0:\n",
        "        updated_value = (c_val * lambda_val * p1_val) / denominator\n",
        "        P1_updated[i1, j] += updated_value\n",
        "        # print('i1:', i1, 'i2:',i2, 'j:', j, 'c_val:',c_val, 'p1_val:', p1_val, 'p2_val:', p2_val, 'lambda_val:',lambda_val,'(c_val * lambda_val * p1_val):', (c_val * lambda_val * p1_val), 'denominator:',denominator,'updated_value:',updated_value)\n",
        "\n",
        "    # # Compute row sums for normalization\n",
        "    # row_sums = P1_updated.sum(axis=1)\n",
        "    # row_sums[row_sums == 0] = 1  # Avoid division by zero\n",
        "    # # print('row', row_sums)\n",
        "    # # print('1', P1_updated.todense())\n",
        "\n",
        "    # # Normalize rows\n",
        "    # P1_updated = P1_updated / row_sums\n",
        "    # # print('2', P1_updated.todense())\n",
        "\n",
        "    # Convert P1_updated to CSR for efficient storage and further operations\n",
        "    P1_updated = P1_updated.tocsr()\n",
        "\n",
        "    return P1_updated\n",
        "\n",
        "\n",
        "def update_p2_sparse(CT, P1_csr, P2_csr, lambda_csr, vocab_size):\n",
        "    # Convert sparse matrices to CSR format for efficient access\n",
        "    # CT_csr = CT.tocsr()\n",
        "    # P1_csr = P1.tocsr()\n",
        "    # P2_csr = P2.tocsr()\n",
        "    # lambda_csr = lambda_matrix.tocsr()\n",
        "\n",
        "    # Initialize P2_updated as a LIL matrix for efficient row-wise operations\n",
        "    P2_updated = sp.lil_matrix((vocab_size, vocab_size))\n",
        "\n",
        "\n",
        "    # Iterate through non-zero elements in CT\n",
        "    CT_nonzero = find(CT)\n",
        "    for i, idx, c_val in zip(*CT_nonzero):\n",
        "        i1 = i\n",
        "        i2, j = get_original_indices(idx, vocab_size)\n",
        "\n",
        "        # Directly access values from CSR matrices\n",
        "        p1_val = P1_csr[i1, j] if P1_csr[i1, j] != 0 else 0\n",
        "        p2_val = P2_csr[i2, j] if P2_csr[i2, j] != 0 else 0\n",
        "        lambda_val = lambda_csr[i1, i2] if lambda_csr[i1, i2] != 0 else 0\n",
        "\n",
        "        # Compute the update for P2 based on the formula\n",
        "        # denominator = lambda_val * p1_val + (1 - lambda_val) * p2_val + 1e-10\n",
        "        denominator2 = lambda_val * p1_val + (1 - lambda_val) * p2_val\n",
        "        denominator = denominator2 if denominator2 != 0 else 1\n",
        "\n",
        "        # denominator[denominator == 0] = 1\n",
        "        # if denominator != 0:\n",
        "        updated_value = (c_val * (1 - lambda_val) * p2_val) / denominator\n",
        "        P2_updated[i2, j] += updated_value\n",
        "\n",
        "\n",
        "    # # Compute row sums for normalization\n",
        "    # row_sums = P2_updated.sum(axis=1)\n",
        "    # row_sums[row_sums == 0] = 1  # Avoid division by zero\n",
        "    # # print('row', row_sums)\n",
        "    # # print('1', P1_updated.todense())\n",
        "\n",
        "    # # Normalize rows\n",
        "    # P2_updated = P2_updated / row_sums\n",
        "    # # print('2', P1_updated.todense())\n",
        "\n",
        "\n",
        "    # Convert P2_updated to CSR for efficient storage and further operations\n",
        "    P2_updated = P2_updated.tocsr()\n",
        "\n",
        "    return P2_updated\n",
        "\n",
        "\n",
        "def update_lambda_sparse(CT, P1_csr, P2_csr, lambda_csr, vocab_size):\n",
        "    # Convert sparse matrices to CSR format for easier access\n",
        "    # CT_csr = CT.tocsr()\n",
        "    # P1_csr = P1.tocsr()\n",
        "    # P2_csr = P2.tocsr()\n",
        "    # lambda_csr = lambda_matrix.tocsr()\n",
        "\n",
        "    # Initialize P1_updated as a LIL matrix for efficient row-wise operations\n",
        "    lambda_updated = sp.lil_matrix((vocab_size, vocab_size))\n",
        "\n",
        "    # Initialize containers to sum contributions to lambda values\n",
        "    sum_c_vals = sp.lil_matrix((vocab_size, vocab_size))\n",
        "    sum_lambda_vals = sp.lil_matrix((vocab_size, vocab_size))\n",
        "\n",
        "    # Iterate through non-zero elements in CT\n",
        "    CT_nonzero = find(CT)\n",
        "    for i, idx, c_val in zip(*CT_nonzero):\n",
        "        i1 = i\n",
        "        i2, j = get_original_indices(idx, vocab_size)\n",
        "\n",
        "        # Extract corresponding non-zero values from P1, P2, and lambda_matrix\n",
        "        # Directly access values from CSR matrices\n",
        "        p1_val = P1_csr[i1, j] if P1_csr[i1, j] != 0 else 0\n",
        "        p2_val = P2_csr[i2, j] if P2_csr[i2, j] != 0 else 0\n",
        "        lambda_val = lambda_csr[i1, i2] if lambda_csr[i1, i2] != 0 else 0\n",
        "\n",
        "        # Compute the denominator for the update formula\n",
        "        # denominator = lambda_val * p1_val + (1 - lambda_val) * p2_val + 1e-10\n",
        "        denominator_ = lambda_val * p1_val + (1 - lambda_val) * p2_val\n",
        "        denominator = denominator_ if denominator_ != 0 else 1\n",
        "\n",
        "        # if denominator != 0:\n",
        "            # Calculate the updates to lambda values\n",
        "        lambda_contribution = (c_val * lambda_val * p1_val) / denominator\n",
        "        sum_c_vals[i1, i2] += c_val\n",
        "        sum_lambda_vals[i1, i2] += lambda_contribution\n",
        "\n",
        "    # Convert LIL matrices back to CSR for final processing\n",
        "    sum_c_vals = sum_c_vals.tocsr()\n",
        "    sum_lambda_vals = sum_lambda_vals.tocsr()\n",
        "\n",
        "    # Direct element-wise operations to update lambda\n",
        "    non_zero_indices = sum_c_vals.nonzero()\n",
        "    for i, j in zip(non_zero_indices[0], non_zero_indices[1]):\n",
        "        # if sum_c_vals[i, j] != 0:\n",
        "          lambda_updated[i, j] = sum_lambda_vals[i, j] / sum_c_vals[i, j]\n",
        "\n",
        "    return lambda_updated\n",
        "\n",
        "\n",
        "# lambda_updated = update_lambda_sparse(CT, P1, P2, lambda_matrix, vocab_size)\n",
        "# print('lambda_updated:\\n', lambda_updated.todense())\n",
        "\n",
        "# # Example usage with dummy sparse matrices\n",
        "# vocab_size = 3\n",
        "# CT = sp.random(3, 9, density=0.3, format='csr', data_rvs=np.ones)\n",
        "# P1 = sp.random(3, 3, density=0.3, format='csr', data_rvs=np.ones)\n",
        "# P2 = sp.random(3, 3, density=0.3, format='csr', data_rvs=np.ones)\n",
        "# lambda_matrix = sp.random(3, 3, density=0.3, format='csr', data_rvs=np.ones)\n",
        "###### initial\n",
        "# CT = C_train_csr_slice2\n",
        "# print('CT:\\n', CT.todense())\n",
        "\n",
        "# # Example usage\n",
        "# P1 = SmartInitialization_p_sparse(C_train_csr, vocab_size)\n",
        "# # print('P1_normalized_sparse:\\n', P1)\n",
        "# print('P1_normalized_dense:\\n', P1.todense())\n",
        "\n",
        "# P2 = SmartInitialization_p_sparse(C_train_csr_slice2, vocab_size)\n",
        "# # print('P2_normalized_sparse:\\n', P2)\n",
        "# print('P2_normalized_sparse:\\n', P2.todense())\n",
        "\n",
        "# lambda_matrix = np.array(\n",
        "#     [[1, 0, 0.3],  # Flattened representation of first \"row\" in 3D space\n",
        "#     [0, 5, 4],  # Flattened representation of second \"row\" in 3D space\n",
        "#     [0.1, 3, 0]]  # Flattened representation of third \"row\" in 3D space\n",
        "#  , dtype = np.float32)\n",
        "\n",
        "# # Set seed for reproducibility\n",
        "# # np.random.seed(42)\n",
        "# # lambda_matrix = np.random.rand(vocab_size, vocab_size)\n",
        "# lambda_matrix = csr_matrix(lambda_matrix)\n",
        "# print('lambda_matrix:\\n',lambda_matrix.todense())\n",
        "\n",
        "# P1_updated = update_p1_sparse(CT, P1, P2, lambda_matrix, vocab_size)\n",
        "# print('P1_updated:\\n', P1_updated.todense())\n",
        "\n",
        "# P2_updated = update_p2_sparse(CT, P1, P2, lambda_matrix, vocab_size)\n",
        "# print('P2_updated:\\n', P2_updated.todense())\n",
        "\n",
        "\n",
        "# update_lambda = update_lambda_sparse(CT, P1, P2, lambda_matrix, vocab_size)\n",
        "# print('update_lambda:\\n', update_lambda.todense())\n",
        "\n",
        "# Clear memory explicitly\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "YYRa8KMVpCHv",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YYRa8KMVpCHv",
        "outputId": "4927e7ec-1614-4c63-bf6f-c3edc84712e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CP_slice: [[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "CP_slice: [[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "iteration: 0\n",
            "iteration: 1\n",
            "iteration: 2\n",
            "iteration: 3\n",
            "iteration update 3\n",
            "CP_slice: [[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "CP_slice: [[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "iteration: 4\n",
            "iteration update 4\n",
            "CP_slice: [[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "CP_slice: [[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "empirical_cross_entropy: 229.33223582100675\n"
          ]
        }
      ],
      "source": [
        "from scipy.sparse import csr_matrix, find\n",
        "import math\n",
        "import gc\n",
        "\n",
        "# alpha = 0.75\n",
        "alpha = 0.75\n",
        "learning_rate = .1  # Learning rate for updates\n",
        "max_iterations = 5\n",
        "rep_lambda = 2\n",
        "\n",
        "\n",
        "###### initial\n",
        "CT = C_train_csr_slice2\n",
        "# print('CT:\\n', CT.todense())\n",
        "\n",
        "# Example usage\n",
        "# P1_initial = SmartInitialization_p_sparse(C_train_csr, vocab_size)\n",
        "\n",
        "def Smoothed_CP(CP, alpha):\n",
        "  A_slice, n_i = Smoothing_P_sparse(CP, alpha)\n",
        "  d_i_sparse = DistinctTokenSparse(CP)\n",
        "  total_missing_mass = alpha * d_i_sparse/n_i\n",
        "  # Convert missing_mass_per_vocab_sparse to a format that can be added to A_slice1\n",
        "  # We're creating a dense array of the same shape as A_slice1 with each row having the same value\n",
        "  missing_mass_per_vocab = np.repeat(total_missing_mass / vocab_size, vocab_size, axis=1)\n",
        "  # Convert the missing_mass_per_vocab to a sparse matrix\n",
        "  missing_mass_per_vocab_sparse = csr_matrix(missing_mass_per_vocab)\n",
        "  # Add the missing_mass_per_vocab_sparse to A_slice1 element-wise (smoothing method to add absolute discount to each element and remove zero element)\n",
        "  P = A_slice + missing_mass_per_vocab_sparse\n",
        "\n",
        "  return P\n",
        "\n",
        "\n",
        "P1 = Smoothed_CP(CP1, alpha)\n",
        "P2 = Smoothed_CP(CP2, alpha)\n",
        "\n",
        "\n",
        "# # print('P1_normalized_sparse:\\n', P1)\n",
        "# # print('P1_normalized_dense:\\n', P1.todense())\n",
        "# print('P2_normalized_sparse:\\n', P2)\n",
        "# print('P2_normalized_sparse:\\n', P2.todense())\n",
        "\n",
        "# lambda_matrix = np.array(\n",
        "#     [[1, 0, 0.3],  # Flattened representation of first \"row\" in 3D space\n",
        "#     [0, 5, 4],  # Flattened representation of second \"row\" in 3D space\n",
        "#     [0.1, 3, 0]]  # Flattened representation of third \"row\" in 3D space\n",
        "#  , dtype = np.float32)\n",
        "\n",
        "# Set seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "lambda_matrix = np.full((vocab_size, vocab_size), 0.88) #0.88\n",
        "# lambda_matrix = np.random.rand(vocab_size, vocab_size)\n",
        "lambda_matrix = csr_matrix(lambda_matrix)\n",
        "# print('lambda_matrix:\\n',lambda_matrix.todense())\n",
        "\n",
        "\n",
        "# C_test = np.array([\n",
        "#     [[1, 5, 0],  # Flattened representation of first \"row\" in 3D space\n",
        "#     [0, 0, 1],  # Flattened representation of second \"row\" in 3D space\n",
        "#     [0, 5, 0]]  # Flattened representation of third \"row\" in 3D space\n",
        "#     ,\n",
        "#     [[2, 3, 0],  # Flattened representation of first \"row\" in 3D space\n",
        "#     [0, 5, 0],  # Flattened representation of second \"row\" in 3D space\n",
        "#     [7, 1, 0]]  # Flattened representation of third \"row\" in 3D space\n",
        "#     ,\n",
        "#     [[2, 0, 0],  # Flattened representation of first \"row\" in 3D space\n",
        "#     [1, 2, 9],  # Flattened representation of second \"row\" in 3D space\n",
        "#     [0, 3, 0]]  # Flattened representation of third \"row\" in 3D space\n",
        "\n",
        "# , ], dtype = np.float32)\n",
        "\n",
        "\n",
        "# # Flatten C_test by combining the last two dimensions\n",
        "# C_test_flattened = C_test.reshape(C_test.shape[0], -1)\n",
        "\n",
        "# # Example of how the flattened structure looks\n",
        "# # print(\"Flattened C_test:\")\n",
        "# # print(C_test_flattened)\n",
        "\n",
        "# # Create a sparse CSR matrix from the flattened structure\n",
        "# # First, find the indices of non-zero elements\n",
        "# rows, cols = np.nonzero(C_test_flattened)\n",
        "# values = C_test_flattened[rows, cols]\n",
        "\n",
        "# # Now, create the CSR matrix\n",
        "# C_test = csr_matrix((values, (rows, cols)), shape=C_test_flattened.shape)\n",
        "# print('C_test_sparse in dense form:\\n', C_test.todense())\n",
        "\n",
        "# d_i_sparse1 = np.array(\n",
        "#  [[2.],\n",
        "#  [1.],\n",
        "#  [5.]])\n",
        "# n_i = np.array(\n",
        "#  [[2.],\n",
        "#  [10.],\n",
        "#  [20.]])\n",
        "# lambda_matrix = np.random.rand(vocab_size, vocab_size)\n",
        "# # print(\"initial_lambda_matrix:\\n\",  lambda_matrix)\n",
        "# # P1 = SmartInitialization_p1(C_train)\n",
        "# # P2 = SmartInitialization_p2(C_train)\n",
        "# P1 = np.random.rand(vocab_size, vocab_size)\n",
        "# P2 = np.random.rand(vocab_size, vocab_size)\n",
        "\n",
        "# def MultiplicativeUpdate(C, lambda_matrix, P1, P2, max_iterations, rep_lambda, learning_rate = 1):\n",
        "\n",
        "\n",
        "def get_empirical_cross_entropy_sparse(C, P1_csr, P2_csr, lambda_csr, N):\n",
        "    empirical_cross_entropy = 0\n",
        "    count = 0\n",
        "    C_nonzero = find(C)\n",
        "    # Assuming C and P_hat can be aligned or P_hat's necessary values can be efficiently computed:\n",
        "    for i, idx, c_val in zip(*C_nonzero):\n",
        "        i2 = i\n",
        "        i1, j = get_original_indices(idx, vocab_size)\n",
        "        # Extract corresponding non-zero values from P1, P2, and lambda_matrix\n",
        "        # Directly access values from CSR matrices\n",
        "        p1_val = P1_csr[i1, j] if P1_csr[i1, j] != 0 else 0\n",
        "        p2_val = P2_csr[i2, j] if P2_csr[i2, j] != 0 else 0\n",
        "        lambda_val = lambda_csr[i1, i2] if lambda_csr[i1, i2] != 0 else 0\n",
        "        # Compute the denominator for the update formula\n",
        "        p_val = lambda_val * p1_val + (1 - lambda_val) * p2_val\n",
        "        count += c_val\n",
        "\n",
        "        if p_val > 0:\n",
        "            empirical_cross_entropy -= c_val * math.log2(p_val)\n",
        "        else:\n",
        "          empirical_cross_entropy -= 0  # Or some default value, or you can raise an error\n",
        "    return math.pow(2, empirical_cross_entropy / N), count\n",
        "\n",
        "\n",
        "for iteration in range(max_iterations):\n",
        "\n",
        "    print('iteration:', iteration)\n",
        "\n",
        "    # Calculate the denominator: sum of C across the last axis (j)\n",
        "    CT = C_train_csr_slice2\n",
        "    # print('CT:\\n', CT.todense())\n",
        "\n",
        "    # # Prevent division by zero by setting zero denominators to 1 (or add a small epsilon)\n",
        "    # denominator_lambda[denominator_lambda == 0] = 1e-10\n",
        "\n",
        "    lambda_updated = update_lambda_sparse(CT, P1, P2, lambda_matrix, vocab_size)\n",
        "    # Compute the scaled difference\n",
        "    scaled_diff = (lambda_updated - lambda_matrix).multiply(learning_rate)\n",
        "    # Compute the new lambda_matrix\n",
        "    lambda_matrix = lambda_matrix + scaled_diff\n",
        "    # print('lambda_matrix111:\\n',lambda_matrix.todense())\n",
        "\n",
        "    # # Optional: Convert back to CSR if you had converted it before\n",
        "    # lambda_matrix = csr_matrix(lambda_matrix_new)\n",
        "\n",
        "\n",
        "    # multipliers = [3,10]  # You can extend this list to include more multipliers\n",
        "    # multipliers = list(range(1,10))\n",
        "    if iteration > rep_lambda:    #only update lambda and P1 and P2 in first iteration and after that iterates only lambda and then after rep_lambda iterates P1 and P2 as well\n",
        "\n",
        "      # if iteration == rep_lambda+1 or iteration == 5*rep_lambda+1 or iteration == 10*rep_lambda+1 or iteration == 15*rep_lambda+1:    #not work\n",
        "      # if iteration == rep_lambda+1 or iteration == 5*rep_lambda+1 or iteration == 10*rep_lambda+1:\n",
        "      # if iteration > rep_lambda and (iteration - 1) % rep_lambda == 0:   #the condition will be true for iterations like rep_lambda + 1, 2*rep_lambda + 1, 3*rep_lambda + 1, and so on.\n",
        "\n",
        "      # if iteration == 0:\n",
        "      #   ## reseting P1 and P2 after rep_lamda so it measns that with smart initialization of P1 and P2 first update lambda with rep_lambda iterations and then reset P1 and P2 since they are good\n",
        "      #   P1 = SmartInitialization_p_sparse(C_train_csr, vocab_size)\n",
        "      #   P2 = SmartInitialization_p_sparse(C_train_csr_slice2, vocab_size)\n",
        "      #   print('iteration initial',iteration)\n",
        "\n",
        "      # if any(iteration == m * rep_lambda + 1 for m in multipliers):\n",
        "      # if iteration == rep_lambda + 1:\n",
        "      #   P1 = Smoothed_CP(CP1, alpha)\n",
        "      #   P2 = Smoothed_CP(CP2, alpha)\n",
        "\n",
        "      #   print('iteration reset',iteration)\n",
        "\n",
        "      # else:\n",
        "\n",
        "        print('iteration update',iteration)\n",
        "\n",
        "        # Update P1\n",
        "        numerator_P1 = update_p1_sparse(CT, P1, P2, lambda_matrix, vocab_size)\n",
        "        P1 = Smoothed_CP(numerator_P1, alpha)\n",
        "\n",
        "        # Update P2\n",
        "        numerator_P2 = update_p2_sparse(CT, P1, P2, lambda_matrix, vocab_size)\n",
        "        P2 = Smoothed_CP(numerator_P2, alpha)\n",
        "\n",
        "        # if iteration % 2 == 0:\n",
        "        #   P_hat = find_P_hat(lambda_matrix, P1, P2)\n",
        "        #   # print('P_hat:', P_hat)\n",
        "        #   N = test_size\n",
        "        #   empirical_cross_entropy = get_empirical_cross_entropy_sparse(C_test, P_hat, N)\n",
        "        #   print('empirical_cross_entropy:', empirical_cross_entropy)\n",
        "            # Clear memory explicitly\n",
        "        gc.collect()\n",
        "\n",
        "\n",
        "# print('lambda_matrix_updated:\\n',lambda_matrix)\n",
        "\n",
        "# print('P1_updated:\\n', P1)\n",
        "# print('P1_updated_dense:\\n', P1.todense())\n",
        "\n",
        "# print('P2_updated:\\n', P2)\n",
        "# print('P2_updated_dense:\\n', P2.todense())\n",
        "\n",
        "# def find_P_hat(lambda_matrix, P1, P2):\n",
        "# # Initialize the 3-dimensional tensor P_hat\n",
        "#   P_hat = np.zeros((vocab_size, vocab_size, vocab_size))  # Tensor for i2, i1, j\n",
        "\n",
        "#   # Populate the tensor P using the provided equation\n",
        "#   for i2 in range(vocab_size):\n",
        "#       for i1 in range(vocab_size):\n",
        "#           for j in range(vocab_size):\n",
        "#               P_hat[i2, i1, j] = lambda_matrix[i1, i2] * P1[i1, j] + (1 - lambda_matrix[i1, i2]) * P2[i2, j]\n",
        "\n",
        "#   # print(\"Tensor P_hat:\\n\", P_hat)\n",
        "#   return P_hat\n",
        "\n",
        "# P_hat = find_P_hat(lambda_matrix, P1, P2)\n",
        "\n",
        "\n",
        "\n",
        "# # Assuming lambda_matrix, P1, P2 are defined sparse matrices\n",
        "# P_hat = find_P_hat(lambda_matrix, P1, P2)\n",
        "\n",
        "# print('-----------------------------------------')\n",
        "\n",
        "N = test_size\n",
        "# N = train_size\n",
        "empirical_cross_entropy, count = get_empirical_cross_entropy_sparse(C_test, P1, P2, lambda_matrix, N)\n",
        "print('empirical_cross_entropy:', empirical_cross_entropy)\n",
        "# print('count:', count)\n",
        "\n",
        "# A_slice1, n_i = Smoothing_P_sparse(numerator_P1, alpha=0.8)\n",
        "# empirical_cross_entropy: 276.6979117176241\n",
        "# emp2 892.2182737491097\n",
        "# count: 67598\n",
        "# N: 81673\n",
        "# test_size: 81673\n",
        "\n",
        "# empirical_cross_entropy: 229.33223582100675\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61d2d590-35b4-4f32-d50a-fb1a13a6d44c",
        "id": "dpgwA_j3bNFS"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CP1:\n",
            " [[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "CP2:\n",
            " [[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n"
          ]
        }
      ],
      "source": [
        "# Creating Sparse bigram C_train and C_test.  I get 252 perplexity with this\n",
        "\n",
        "from scipy.sparse import coo_array\n",
        "import numpy as np\n",
        "\n",
        "def CreateCP1(data, word_to_index, vocab_size, slice_='True'):\n",
        "    # Lists to accumulate the non-zero values and their coordinates\n",
        "    values = []  # Non-zero data entries\n",
        "    row_indices = []  # Row coordinates\n",
        "    col_indices = []  # Column coordinates (flattened last two dimensions)\n",
        "\n",
        "    # Process the input data to populate 'values' and 'coords'\n",
        "    for i in range(len(data) - 1):\n",
        "        word1, word2 = data[i], data[i + 1]\n",
        "        if word1 in word_to_index and word2 in word_to_index:\n",
        "            i2 = word_to_index[word1]\n",
        "            i1 = word_to_index[word2]\n",
        "\n",
        "            # Append the data value (1 for each trigram occurrence)\n",
        "            values.append(1)\n",
        "\n",
        "            if slice_ == 'True':\n",
        "              # Append the row and column indices\n",
        "              row_indices.append(i2)\n",
        "              col_indices.append(i1)\n",
        "            # else:\n",
        "            #   row_indices.append(i1)\n",
        "            #   col_indices.append(i2 * vocab_size + j)  # Flatten the 2D indices into 1D\n",
        "\n",
        "    # Convert lists to numpy arrays\n",
        "    data_np = np.array(values)\n",
        "    row_np = np.array(row_indices)\n",
        "    col_np = np.array(col_indices)\n",
        "\n",
        "    # Create the sparse COO matrix\n",
        "    tensor_coo = coo_array((data_np, (row_np, col_np)), shape=(vocab_size, vocab_size), dtype=np.float32)\n",
        "    tensor_csr = tensor_coo.tocsr()         # Removing Duplicates by Converting Formats\n",
        "    # # Note: Create the sparse CSR matrix, Using vocab_size**2 for the number of columns as the last two dimensions are flattened\n",
        "    # C_csr = csr_matrix((values, (rows, cols)), shape=(vocab_size, vocab_size**2))\n",
        "\n",
        "    return tensor_csr\n",
        "\n",
        "\n",
        "def CreateCP2(data, word_to_index, vocab_size, slice_='True'):\n",
        "    # Lists to accumulate the non-zero values and their coordinates\n",
        "    values = []  # Non-zero data entries\n",
        "    row_indices = []  # Row coordinates\n",
        "    col_indices = []  # Column coordinates (flattened last two dimensions)\n",
        "\n",
        "    # Process the input data to populate 'values' and 'coords'\n",
        "    for i in range(len(data) - 2):\n",
        "        word1, word2, word3 = data[i], data[i + 1], data[i + 2]\n",
        "        if word1 in word_to_index and word2 in word_to_index and word3 in word_to_index:\n",
        "            i2 = word_to_index[word1]\n",
        "            i1 = word_to_index[word2]\n",
        "            j = word_to_index[word3]\n",
        "\n",
        "            # Append the data value (1 for each trigram occurrence)\n",
        "            values.append(1)\n",
        "\n",
        "            if slice_ == 'True':\n",
        "              # Append the row and column indices\n",
        "              row_indices.append(i2)\n",
        "              col_indices.append(j)\n",
        "            # else:\n",
        "            #   row_indices.append(i1)\n",
        "            #   col_indices.append(i2 * vocab_size + j)  # Flatten the 2D indices into 1D\n",
        "\n",
        "    # Convert lists to numpy arrays\n",
        "    data_np = np.array(values)\n",
        "    row_np = np.array(row_indices)\n",
        "    col_np = np.array(col_indices)\n",
        "\n",
        "    # Create the sparse COO matrix\n",
        "    tensor_coo = coo_array((data_np, (row_np, col_np)), shape=(vocab_size, vocab_size), dtype=np.float32)\n",
        "    tensor_csr = tensor_coo.tocsr()         # Removing Duplicates by Converting Formats\n",
        "    # # Note: Create the sparse CSR matrix, Using vocab_size**2 for the number of columns as the last two dimensions are flattened\n",
        "    # C_csr = csr_matrix((values, (rows, cols)), shape=(vocab_size, vocab_size**2))\n",
        "\n",
        "    return tensor_csr\n",
        "\n",
        "\n",
        "\n",
        "#-------------------------------------\n",
        "# Example usage:\n",
        "# vocab = [\"0\", \"1\", \"2\"]\n",
        "# train_data_all = [\"0\", \"1\", \"2\", \"0\", \"1\", \"2\",\"1\",\"0\",\"1\"]\n",
        "# test_data_all = [\"0\", \"0\", \"2\", \"0\", \"2\"]\n",
        "# def generate_trigrams(data):\n",
        "#     # Create trigrams by zipping over the data offset by one word each time\n",
        "#     return list(zip(data, data[1:], data[2:]))\n",
        "\n",
        "# test_trigrams = generate_trigrams(test_data_all)\n",
        "# test_size = len(test_trigrams)\n",
        "\n",
        "\n",
        "# def generate_bigrams(data):\n",
        "#     # Create bigrams by zipping over the data offset by one word each time\n",
        "#     return list(zip(data, data[1:]))\n",
        "\n",
        "# test_bigrams = generate_bigrams(test_data_all)\n",
        "# test_size = len(test_bigrams)\n",
        "#-------------------------------------\n",
        "\n",
        "\n",
        "word_to_index = {word: index for index, word in enumerate(vocab)}\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "CP1 = CreateCP1(train_data_all, word_to_index, vocab_size,'True')\n",
        "print('CP1:\\n', CP1.todense())\n",
        "\n",
        "\n",
        "CP2 = CreateCP2(train_data_all, word_to_index, vocab_size,'True')\n",
        "print('CP2:\\n', CP2.todense())\n",
        "\n",
        "# C_train_csr_slice2 = CreateSparseTensor(train_data_all, word_to_index, vocab_size,'False')\n",
        "# print('C_train_csr_dense_slice2:\\n', C_train_csr_slice2.todense())\n",
        "\n",
        "# CP1_test = CreateCP1(test_data_all, word_to_index, vocab_size,'True')\n",
        "# print('CP1_test:\\n', CP1_test.todense())\n",
        "\n",
        "# print('test_size:',test_size)\n",
        "\n",
        "# print('C_train_csr\\n:', C_train_csr)\n",
        "# print('C_train_csr_dense:\\n', C_train_csr.todense())\n",
        "# print('sparse.nnz:\\n', C_train_csr.nnz)\n",
        "# print('sum on rows\\n:', C_train_csr.sum(axis=0))\n",
        "# print('sum on columns\\n:', C_train_csr.sum(axis=1))\n",
        "# print('data\\n:', C_train_csr.data)\n",
        "# # print('slicing_dense:\\n', C_train_csr.todense()[[1], :])\n",
        "# # print('slicing_sparse:\\n', C_train_csr.tocsr()[[1], :])\n",
        "# # print('transpose_dense:\\n', C_train_csr.T.todense())\n",
        "# # print('transpose_sparse:\\n', C_train_csr.T)\n",
        "# # print('remove zeros:\\n', C_train_csr.tocsr().eliminate_zeros())\n",
        "# print('p1\\n:', C_train_csr.sum(axis=0))\n",
        "# print(type(C_train_csr))\n",
        "\n",
        "# lambda_matrix = np.array(\n",
        "#     [[1, 0, 0.3],  # Flattened representation of first \"row\" in 3D space\n",
        "#     [0, 5, 4],  # Flattened representation of second \"row\" in 3D space\n",
        "#     [0.1, 3, 0]]  # Flattened representation of third \"row\" in 3D space\n",
        "#  , dtype = np.float32)\n",
        "# lambda_matrix = csr_matrix(lambda_matrix)\n",
        "# print('lambda_matrix:\\n',lambda_matrix.todense())\n"
      ],
      "id": "dpgwA_j3bNFS"
    },
    {
      "cell_type": "code",
      "source": [
        "lambda_matrix.max()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "id": "jM2rAPjxk_x9",
        "outputId": "8151c460-6fef-4605-ed28-0e7958d13b7e"
      },
      "id": "jM2rAPjxk_x9",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'lambda_matrix' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-f8b8b868a750>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlambda_matrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'lambda_matrix' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lambda_matrix.min()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I68ifMGKlCsW",
        "outputId": "f2a1bd80-7760-4a24-887f-2f60d58ee53a"
      },
      "id": "I68ifMGKlCsW",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.792"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Compute the mean\n",
        "matrix = lambda_matrix\n",
        "mean = np.mean(matrix)\n",
        "\n",
        "# Step 2: Compute the squared differences\n",
        "squared_diff = (matrix - mean) ** 2\n",
        "\n",
        "# Step 3: Compute the variance\n",
        "variance = np.mean(squared_diff)\n",
        "\n",
        "# Step 4: Compute the standard deviation\n",
        "std_deviation = np.sqrt(variance)\n",
        "\n",
        "print(\"Standard deviation of the matrix:\", std_deviation)"
      ],
      "metadata": {
        "id": "hk_peQadlUmL"
      },
      "id": "hk_peQadlUmL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.sparse import csr_matrix, find\n",
        "import math\n",
        "import gc\n",
        "\n",
        "alpha = 0.75\n",
        "learning_rate = .1  # Learning rate for updates\n",
        "max_iterations = 0\n",
        "rep_lambda = 5\n",
        "\n",
        "\n",
        "P1_initial = CP1\n",
        "A_slice1, n_i = Smoothing_P_sparse(P1_initial, alpha=0.8)\n",
        "# print('n_i:', type(n_i))\n",
        "# if np.any(n_i != 1):\n",
        "#     print(\"There is at least one element in the matrix that is not equal to 1.\")\n",
        "# else:\n",
        "#     print(\"All elements in the matrix are equal to 1.\")\n",
        "d_i_sparse1 = DistinctTokenSparse(P1_initial)\n",
        "total_missing_mass1 = alpha * d_i_sparse1/n_i\n",
        "\n",
        "# Convert missing_mass_per_vocab_sparse to a format that can be added to A_slice1\n",
        "# We're creating a dense array of the same shape as A_slice1 with each row having the same value\n",
        "missing_mass_per_vocab = np.repeat(total_missing_mass1 / vocab_size, vocab_size, axis=1)\n",
        "# Convert the missing_mass_per_vocab to a sparse matrix\n",
        "missing_mass_per_vocab_sparse = csr_matrix(missing_mass_per_vocab)\n",
        "\n",
        "# Add the missing_mass_per_vocab_sparse to A_slice1 element-wise (smoothing method to add absolute discount to each element and remove zero element)\n",
        "P1 = A_slice1 + missing_mass_per_vocab_sparse\n",
        "\n",
        "\n",
        "def get_empirical_cross_entropy_sparse(C, P1_csr, N):\n",
        "    empirical_cross_entropy = 0\n",
        "    count = 0\n",
        "    C_nonzero = find(C)\n",
        "    # Assuming C and P_hat can be aligned or P_hat's necessary values can be efficiently computed:\n",
        "    for i, idx, c_val in zip(*C_nonzero):\n",
        "        i2 = i\n",
        "        i1 = idx\n",
        "        # Extract corresponding non-zero values from P1, P2, and lambda_matrix\n",
        "        # Directly access values from CSR matrices\n",
        "        p1_val = P1_csr[i2, i1]\n",
        "        # p2_val = P2_csr[i2, j] if P2_csr[i2, j] != 0 else 0\n",
        "        # lambda_val = lambda_csr[i1, i2] if lambda_csr[i1, i2] != 0 else 0\n",
        "        # Compute the denominator for the update formula\n",
        "        # p_val = lambda_val * p1_val + (1 - lambda_val) * p2_val\n",
        "        p_val = p1_val\n",
        "        count += c_val\n",
        "\n",
        "        if p_val > 0:\n",
        "            empirical_cross_entropy -= c_val * math.log2(p_val)\n",
        "        else:\n",
        "          empirical_cross_entropy -= 0  # Or some default value, or you can raise an error\n",
        "    return math.pow(2, empirical_cross_entropy / N), count\n",
        "\n",
        "\n",
        "N = test_size2\n",
        "# N = train_size\n",
        "empirical_cross_entropy, count = get_empirical_cross_entropy_sparse(CP1, P1, N)\n",
        "print('empirical_cross_entropy:', empirical_cross_entropy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PWKpMUkLrwOZ",
        "outputId": "fbafa71a-150b-410e-b515-c12d28588e19"
      },
      "id": "PWKpMUkLrwOZ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CP_slice: [[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "empirical_cross_entropy: 3.607668151910993e+22\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1572f8be",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1572f8be",
        "outputId": "3f390188-7142-4c0d-f815-a7dbc970f24d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "P1_normalized_dense:\n",
            " [[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "P2_normalized_sparse:\n",
            " [[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n"
          ]
        }
      ],
      "source": [
        "# final code for sparse p1 and p2\n",
        "\n",
        "from scipy.sparse import csr_matrix\n",
        "import numpy as np\n",
        "\n",
        "# print('lambda_matrix:\\n',lambda_matrix.todense())\n",
        "# print('C_train_csr_dense:\\n', C_train_csr.todense())\n",
        "\n",
        "def SmartInitialization_p_sparse(C_train_csr, vocab_size):\n",
        "    \"\"\"\n",
        "    Normalizes a sparse CSR matrix by summing along axis=0 and then normalizing along a specific dimension.\n",
        "\n",
        "    :param C_train_csr: Sparse matrix in CSR format representing summed data.\n",
        "    :param vocab_size: The size of the vocabulary, used for understanding the structure.\n",
        "    :return: A sparse CSR matrix where each row is normalized by its sum.\n",
        "    \"\"\"\n",
        "    # Sum along columns to collapse the matrix into a single row (vocab_size, )\n",
        "    P1_sum = C_train_csr.sum(axis=0)\n",
        "    # print('P1_sum =', P1_sum)\n",
        "\n",
        "    # For a CSR matrix, P1_sum here is a 1xN matrix. We need to convert it to a dense format for further manipulation.\n",
        "    # Note: This conversion to dense is necessary for division and reshaping operations that follow.\n",
        "    P1_dense = np.array(P1_sum).squeeze()  # Convert to dense and flatten\n",
        "    # print('P1_dense =', P1_dense)\n",
        "\n",
        "    # Reshape P1_dense back into the (vocab_size, vocab_size) structure\n",
        "    # This step assumes that the vocab_size**2 matches the total number of columns in C_train_csr\n",
        "    P1_reshaped = P1_dense.reshape(vocab_size, vocab_size)\n",
        "    # print('P1_reshaped =', P1_reshaped)\n",
        "\n",
        "    # Compute row sums for normalization\n",
        "    row_sums = np.sum(P1_reshaped, axis=1, keepdims=True)\n",
        "    row_sums[row_sums == 0] = 1  # Avoid division by zero\n",
        "\n",
        "    # Normalize rows\n",
        "    P1_normalized = P1_reshaped / row_sums\n",
        "\n",
        "    # If needed, convert P1_normalized back to a sparse matrix\n",
        "    # Note: Depending on subsequent use, you might keep it dense or convert back to sparse\n",
        "    P1_sparse_normalized = csr_matrix(P1_normalized)\n",
        "    # P1_sparse_normalized = P1_normalized\n",
        "\n",
        "    return P1_sparse_normalized\n",
        "\n",
        "# Example usage\n",
        "P1_normalized_sparse = SmartInitialization_p_sparse(C_train_csr, vocab_size)\n",
        "# print('P1_normalized_sparse:\\n', P1_normalized_sparse)\n",
        "print('P1_normalized_dense:\\n', P1_normalized_sparse.todense())\n",
        "\n",
        "P2_normalized_sparse = SmartInitialization_p_sparse(C_train_csr_slice2, vocab_size)\n",
        "print('P2_normalized_sparse:\\n', P2_normalized_sparse.todense())\n",
        "# print('P2_normalized_sparse:\\n', P2_normalized_sparse)\n",
        "\n",
        "# C_train_csr_dense:\n",
        "#  [[0. 0. 0. 0. 0. 2. 0. 0. 0.]\n",
        "#  [0. 1. 0. 0. 0. 0. 1. 1. 0.]\n",
        "#  [0. 1. 0. 1. 0. 0. 0. 0. 0.]]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cnfAawYG6qJ8",
      "metadata": {
        "id": "cnfAawYG6qJ8"
      },
      "source": [
        "the main code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9023eb8e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9023eb8e",
        "outputId": "37d0679a-c1d2-4dc0-8f29-69f0928d6802"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Correct one\n",
        "import numpy as np\n",
        "import gc\n",
        "import scipy.sparse as sp\n",
        "\n",
        "# # Assuming CT, P1, P2, and lambda_matrix are already in CSR format\n",
        "# CT = C_train_csr_slice2\n",
        "# print('CT:\\n', CT.todense())\n",
        "# # Set seed for reproducibility\n",
        "# np.random.seed(42)\n",
        "# lambda_matrix = np.random.rand(vocab_size, vocab_size)\n",
        "# lambda_matrix = csr_matrix(lambda_matrix)\n",
        "# P1 = P1_normalized_sparse\n",
        "# P2 = P2_normalized_sparse\n",
        "\n",
        "def get_original_indices(flat_idx, vocab_size):\n",
        "    \"\"\"Convert flat index back to (i2, j) index pair\"\"\"\n",
        "    i2 = flat_idx // vocab_size\n",
        "    j = flat_idx % vocab_size\n",
        "    return i2, j\n",
        "\n",
        "\n",
        "def update_p1_sparse(CT, P1_csr, P2_csr, lambda_csr, vocab_size):\n",
        "    # Ensure all matrices are in CSR format for efficient row access and arithmetic operations\n",
        "    # CT_csr = CT.tocsr()\n",
        "    # P1_csr = P1.tocsr()\n",
        "    # P2_csr = P2.tocsr()\n",
        "    # lambda_csr = lambda_matrix.tocsr()\n",
        "\n",
        "\n",
        "    # Initialize P1_updated as a LIL matrix for efficient row-wise operations\n",
        "    P1_updated = sp.lil_matrix((vocab_size, vocab_size))\n",
        "\n",
        "    # Iterate through non-zero elements in CT\n",
        "    CT_nonzero = find(CT)\n",
        "    for i, idx, c_val in zip(*CT_nonzero):\n",
        "        i1 = i\n",
        "        i2, j = get_original_indices(idx, vocab_size)\n",
        "\n",
        "        # Directly extract values from CSR matrices\n",
        "        p1_val = P1_csr[i1, j] if P1_csr[i1, j] != 0 else 0\n",
        "        p2_val = P2_csr[i2, j] if P2_csr[i2, j] != 0 else 0\n",
        "        lambda_val = lambda_csr[i1, i2] if lambda_csr[i1, i2] != 0 else 0\n",
        "\n",
        "        # Compute the update for P1 based on the formula\n",
        "        denominator = lambda_val * p1_val + (1 - lambda_val) * p2_val + 1e-10\n",
        "        # if denominator != 0:\n",
        "        updated_value = (c_val * lambda_val * p1_val) / denominator\n",
        "        P1_updated[i1, j] += updated_value\n",
        "\n",
        "    # Convert P1_updated to CSR for efficient storage and further operations\n",
        "    P1_updated = P1_updated.tocsr()\n",
        "\n",
        "    return P1_updated\n",
        "\n",
        "\n",
        "def update_p2_sparse(CT, P1_csr, P2_csr, lambda_csr, vocab_size):\n",
        "    # Convert sparse matrices to CSR format for efficient access\n",
        "    # CT_csr = CT.tocsr()\n",
        "    # P1_csr = P1.tocsr()\n",
        "    # P2_csr = P2.tocsr()\n",
        "    # lambda_csr = lambda_matrix.tocsr()\n",
        "\n",
        "    # Initialize P2_updated as a LIL matrix for efficient row-wise operations\n",
        "    P2_updated = sp.lil_matrix((vocab_size, vocab_size))\n",
        "\n",
        "    # Iterate through non-zero elements in CT\n",
        "    CT_nonzero = find(CT)\n",
        "    for i, idx, c_val in zip(*CT_nonzero):\n",
        "        i1 = i\n",
        "        i2, j = get_original_indices(idx, vocab_size)\n",
        "\n",
        "        # Directly access values from CSR matrices\n",
        "        p1_val = P1_csr[i1, j] if P1_csr[i1, j] != 0 else 0\n",
        "        p2_val = P2_csr[i2, j] if P2_csr[i2, j] != 0 else 0\n",
        "        lambda_val = lambda_csr[i1, i2] if lambda_csr[i1, i2] != 0 else 0\n",
        "\n",
        "        # Compute the update for P2 based on the formula\n",
        "        denominator = lambda_val * p1_val + (1 - lambda_val) * p2_val + 1e-10\n",
        "        # if denominator != 0:\n",
        "        updated_value = (c_val * (1 - lambda_val) * p2_val) / denominator\n",
        "        P2_updated[i2, j] += updated_value\n",
        "\n",
        "    # Convert P2_updated to CSR for efficient storage and further operations\n",
        "    P2_updated = P2_updated.tocsr()\n",
        "\n",
        "    return P2_updated\n",
        "\n",
        "\n",
        "def update_lambda_sparse(CT, P1_csr, P2_csr, lambda_csr, vocab_size):\n",
        "    # Convert sparse matrices to CSR format for easier access\n",
        "    # CT_csr = CT.tocsr()\n",
        "    # P1_csr = P1.tocsr()\n",
        "    # P2_csr = P2.tocsr()\n",
        "    # lambda_csr = lambda_matrix.tocsr()\n",
        "\n",
        "    # Initialize P1_updated as a LIL matrix for efficient row-wise operations\n",
        "    lambda_updated = sp.lil_matrix((vocab_size, vocab_size))\n",
        "\n",
        "    # Initialize containers to sum contributions to lambda values\n",
        "    sum_c_vals = sp.lil_matrix((vocab_size, vocab_size))\n",
        "    sum_lambda_vals = sp.lil_matrix((vocab_size, vocab_size))\n",
        "\n",
        "    # Iterate through non-zero elements in CT\n",
        "    CT_nonzero = find(CT)\n",
        "    for i, idx, c_val in zip(*CT_nonzero):\n",
        "        i1 = i\n",
        "        i2, j = get_original_indices(idx, vocab_size)\n",
        "\n",
        "        # Extract corresponding non-zero values from P1, P2, and lambda_matrix\n",
        "        # Directly access values from CSR matrices\n",
        "        p1_val = P1_csr[i1, j] if P1_csr[i1, j] != 0 else 0\n",
        "        p2_val = P2_csr[i2, j] if P2_csr[i2, j] != 0 else 0\n",
        "        lambda_val = lambda_csr[i1, i2] if lambda_csr[i1, i2] != 0 else 0\n",
        "\n",
        "        # Compute the denominator for the update formula\n",
        "        denominator = lambda_val * p1_val + (1 - lambda_val) * p2_val + 1e-10\n",
        "        # if denominator != 0:\n",
        "            # Calculate the updates to lambda values\n",
        "        lambda_contribution = (c_val * lambda_val * p1_val) / denominator\n",
        "        sum_c_vals[i1, i2] += c_val\n",
        "        sum_lambda_vals[i1, i2] += lambda_contribution\n",
        "\n",
        "    # Convert LIL matrices back to CSR for final processing\n",
        "    sum_c_vals = sum_c_vals.tocsr()\n",
        "    sum_lambda_vals = sum_lambda_vals.tocsr()\n",
        "\n",
        "    # Direct element-wise operations to update lambda\n",
        "    non_zero_indices = sum_c_vals.nonzero()\n",
        "    for i, j in zip(non_zero_indices[0], non_zero_indices[1]):\n",
        "        # if sum_c_vals[i, j] != 0:\n",
        "          lambda_updated[i, j] = sum_lambda_vals[i, j] / sum_c_vals[i, j]\n",
        "\n",
        "    return lambda_updated\n",
        "\n",
        "\n",
        "# lambda_updated = update_lambda_sparse(CT, P1, P2, lambda_matrix, vocab_size)\n",
        "# print('lambda_updated:\\n', lambda_updated.todense())\n",
        "\n",
        "# # Example usage with dummy sparse matrices\n",
        "# vocab_size = 3\n",
        "# CT = sp.random(3, 9, density=0.3, format='csr', data_rvs=np.ones)\n",
        "# P1 = sp.random(3, 3, density=0.3, format='csr', data_rvs=np.ones)\n",
        "# P2 = sp.random(3, 3, density=0.3, format='csr', data_rvs=np.ones)\n",
        "# lambda_matrix = sp.random(3, 3, density=0.3, format='csr', data_rvs=np.ones)\n",
        "\n",
        "# P1_updated = update_p1_sparse(CT, P1, P2, lambda_matrix, vocab_size)\n",
        "# print('P1_updated:\\n', P1_updated.todense())\n",
        "\n",
        "# P2_updated = update_p2_sparse(CT, P1, P2, lambda_matrix, vocab_size)\n",
        "# print('P2_updated:\\n', P2_updated.todense())\n",
        "# Clear memory explicitly\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RkmpqhWluBm9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "id": "RkmpqhWluBm9",
        "outputId": "1b1ee1e4-9b0c-4ae3-bdbb-c4ccd8f144d1"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-78606830ce4e>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlambda_updated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate_lambda_sparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mP1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mP2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# Compute the scaled difference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mscaled_diff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlambda_updated\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlambda_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Compute the new lambda_matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlambda_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlambda_matrix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mscaled_diff\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-7fd821387265>\u001b[0m in \u001b[0;36mupdate_lambda_sparse\u001b[0;34m(CT, P1_csr, P2_csr, lambda_csr, vocab_size)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;31m# Extract corresponding non-zero values from P1, P2, and lambda_matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;31m# Directly access values from CSR matrices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m         \u001b[0mp1_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mP1_csr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mP1_csr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0mp2_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mP2_csr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mP2_csr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0mlambda_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlambda_csr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlambda_csr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/sparse/_index.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mINT_TYPES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mINT_TYPES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_intXint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_on_1d_array_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/sparse/_compressed.py\u001b[0m in \u001b[0;36m_get_intXint\u001b[0;34m(self, row, col)\u001b[0m\n\u001b[1;32m    654\u001b[0m         \u001b[0mM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_swap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    655\u001b[0m         \u001b[0mmajor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_swap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 656\u001b[0;31m         indptr, indices, data = get_csr_submatrix(\n\u001b[0m\u001b[1;32m    657\u001b[0m             \u001b[0mM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindptr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m             major, major + 1, minor, minor + 1)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "P1 = SmartInitialization_p_sparse(C_train_csr, vocab_size)\n",
        "print('P1_normalized_dense:\\n',P1_normalized_sparse.todense())\n",
        "P2 = SmartInitialization_p_sparse(C_train_csr_slice2, vocab_size)\n",
        "print('P2_normalized_dense:\\n',P2_normalized_sparse.todense())\n",
        "\n",
        "lambda_updated = update_lambda_sparse(CT, P1, P2, lambda_matrix, vocab_size)\n",
        "# Compute the scaled difference\n",
        "scaled_diff = (lambda_updated - lambda_matrix).multiply(learning_rate)\n",
        "# Compute the new lambda_matrix\n",
        "lambda_matrix = lambda_matrix + scaled_diff"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Q20mJvACuUQu",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q20mJvACuUQu",
        "outputId": "289d329d-b974-4578-bfe1-5298e796679f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "P1_normalized_dense:\n",
            " [[0.         1.         0.        ]\n",
            " [0.33333334 0.         0.6666667 ]\n",
            " [0.5        0.5        0.        ]]\n",
            "numerator_P1:\n",
            " [[0.00000000e+00 7.63021894e-01 0.00000000e+00]\n",
            " [6.53284855e-04 0.00000000e+00 4.35838714e-02]\n",
            " [4.04962378e-01 2.53888919e-01 0.00000000e+00]]\n",
            "A_slice1:\n",
            " [[0.         0.2        0.        ]\n",
            " [0.00295356 0.         0.19704644]\n",
            " [0.12292983 0.07707017 0.        ]] \n",
            "n_i:\n",
            " [[0.76302189]\n",
            " [0.04423716]\n",
            " [0.6588513 ]]\n",
            "d_i_sparse1:\n",
            " [[0.76302189]\n",
            " [0.04423716]\n",
            " [0.6588513 ]]\n",
            "total_missing_mass1:\n",
            " [[0.8]\n",
            " [0.8]\n",
            " [0.8]]\n",
            "P11111:\n",
            " [[0.26666667 0.46666667 0.26666667]\n",
            " [0.26962022 0.26666667 0.46371311]\n",
            " [0.38959649 0.34373684 0.26666667]]\n"
          ]
        }
      ],
      "source": [
        "# Update P1\n",
        "alpha = 0.8\n",
        "learning_rate = .1  # Learning rate for updates\n",
        "CT = C_train_csr_slice2\n",
        "\n",
        "P1 = SmartInitialization_p_sparse(C_train_csr, vocab_size)\n",
        "print('P1_normalized_dense:\\n',P1_normalized_sparse.todense())\n",
        "P2 = SmartInitialization_p_sparse(C_train_csr_slice2, vocab_size)\n",
        "# print('P2_normalized_dense:\\n',P2_normalized_sparse.todense())\n",
        "\n",
        "numerator_P1 = update_p1_sparse(CT, P1, P2, lambda_matrix, vocab_size)\n",
        "print('numerator_P1:\\n',numerator_P1.todense())\n",
        "A_slice1, n_i = Smoothing_P_sparse(numerator_P1, alpha=0.8)\n",
        "print('A_slice1:\\n',A_slice1.todense(), '\\nn_i:\\n', n_i)\n",
        "d_i_sparse1 = DistinctTokenSparse(numerator_P1)\n",
        "print('d_i_sparse1:\\n',d_i_sparse1)\n",
        "total_missing_mass1 = alpha * d_i_sparse1/n_i\n",
        "print('total_missing_mass1:\\n',total_missing_mass1)\n",
        "\n",
        "# Convert missing_mass_per_vocab_sparse to a format that can be added to A_slice1\n",
        "# We're creating a dense array of the same shape as A_slice1 with each row having the same value\n",
        "missing_mass_per_vocab = np.repeat(total_missing_mass1 / vocab_size, vocab_size, axis=1)\n",
        "# Convert the missing_mass_per_vocab to a sparse matrix\n",
        "missing_mass_per_vocab_sparse = csr_matrix(missing_mass_per_vocab)\n",
        "\n",
        "# Add the missing_mass_per_vocab_sparse to A_slice1 element-wise (smoothing method to add absolute discount to each element and remove zero element)\n",
        "P1 = A_slice1 + missing_mass_per_vocab_sparse\n",
        "print('P11111:\\n',P1.todense())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "L2R8qDzPua4n",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L2R8qDzPua4n",
        "outputId": "1727cdbc-1f83-4c07-e122-f75929388626"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "P2_normalized_dense:\n",
            " [[0.         0.         1.        ]\n",
            " [0.33333334 0.6666667  0.        ]\n",
            " [0.5        0.5        0.        ]]\n",
            "CT:\n",
            " [[0. 0. 0. 0. 1. 0. 0. 1. 0.]\n",
            " [0. 0. 2. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 1. 1. 0. 0. 0. 0.]]\n",
            "numerator_P2:\n",
            " [[0.         0.         1.95641613]\n",
            " [0.59503762 1.3566861  0.        ]\n",
            " [0.99934671 0.62640309 0.        ]]\n",
            "A_slice2:\n",
            " [[0.         0.         0.59108904]\n",
            " [0.0609756  0.28522792 0.        ]\n",
            " [0.12293979 0.07706021 0.        ]] \n",
            "n_i:\n",
            " [[1.95641613]\n",
            " [1.95172372]\n",
            " [1.6257498 ]]\n",
            "d_i_sparse2:\n",
            " [[1.        ]\n",
            " [1.59503762]\n",
            " [1.6257498 ]]\n",
            "total_missing_mass2:\n",
            " [[0.40891096]\n",
            " [0.65379648]\n",
            " [0.8       ]]\n",
            "P22222:\n",
            " [[0.13630365 0.13630365 0.72739269]\n",
            " [0.27890776 0.50316008 0.21793216]\n",
            " [0.38960646 0.34372687 0.26666667]]\n"
          ]
        }
      ],
      "source": [
        "# # Update P2\n",
        "\n",
        "P1 = SmartInitialization_p_sparse(C_train_csr, vocab_size)\n",
        "# print('P1_normalized_dense:\\n',P1_normalized_sparse.todense())\n",
        "P2 = SmartInitialization_p_sparse(C_train_csr_slice2, vocab_size)\n",
        "print('P2_normalized_dense:\\n',P2_normalized_sparse.todense())\n",
        "CT = C_train_csr_slice2\n",
        "print('CT:\\n',CT.todense())\n",
        "\n",
        "numerator_P2 = update_p2_sparse(CT, P1, P2, lambda_matrix, vocab_size)\n",
        "print('numerator_P2:\\n',numerator_P2.todense())\n",
        "A_slice2, n_i = Smoothing_P_sparse(numerator_P2, alpha=0.8)\n",
        "print('A_slice2:\\n',A_slice2.todense(), '\\nn_i:\\n', n_i)\n",
        "d_i_sparse2 = DistinctTokenSparse(numerator_P2)\n",
        "print('d_i_sparse2:\\n',d_i_sparse2)\n",
        "total_missing_mass2 = alpha * d_i_sparse2/n_i\n",
        "print('total_missing_mass2:\\n',total_missing_mass2)\n",
        "# Convert missing_mass_per_vocab_sparse to a format that can be added to A_slice1\n",
        "# We're creating a dense array of the same shape as A_slice1 with each row having the same value\n",
        "missing_mass_per_vocab2 = np.repeat(total_missing_mass2 / vocab_size, vocab_size, axis=1)\n",
        "# Convert the missing_mass_per_vocab to a sparse matrix\n",
        "missing_mass_per_vocab_sparse2 = csr_matrix(missing_mass_per_vocab2)\n",
        "P2 = A_slice2 + missing_mass_per_vocab_sparse2\n",
        "print('P22222:\\n',P2.todense())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "U-gH7LuJmEOY",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U-gH7LuJmEOY",
        "outputId": "d7b6b8eb-34f1-46fc-d7ad-4c9ff8cc0760"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "lambda_matrix:\n",
            " [[0.37454012 0.95071431 0.73199394 ... 0.83522929 0.89345905 0.82322636]\n",
            " [0.10304169 0.22744825 0.00312165 ... 0.71613506 0.73364407 0.93356231]\n",
            " [0.20907129 0.99456557 0.7734812  ... 0.90922276 0.30271467 0.75398341]\n",
            " ...\n",
            " [0.5504548  0.48085988 0.69397494 ... 0.26938843 0.95902593 0.66106117]\n",
            " [0.59110639 0.4921623  0.23967714 ... 0.76942664 0.8686995  0.77491025]\n",
            " [0.7822393  0.62603131 0.55499552 ... 0.25877062 0.79041964 0.40993842]]\n",
            "iteration: 0\n",
            "iteration initial 0\n",
            "iteration: 1\n",
            "iteration: 2\n",
            "iteration update 2\n",
            "P11111:\n",
            " [[4.61194501e-06 4.61194501e-06 4.61194501e-06 ... 4.61194501e-06\n",
            "  4.61194501e-06 4.61194501e-06]\n",
            " [2.93494020e-05 2.93494020e-05 2.93494020e-05 ... 2.93494020e-05\n",
            "  2.93494020e-05 2.93494020e-05]\n",
            " [5.36513647e-05 5.36513647e-05 5.36513647e-05 ... 5.36513647e-05\n",
            "  5.36513647e-05 5.36513647e-05]\n",
            " ...\n",
            " [8.05558353e-05 8.05558353e-05 8.05558353e-05 ... 8.05558353e-05\n",
            "  8.05558353e-05 8.05558353e-05]\n",
            " [8.05558353e-05 8.05558353e-05 8.05558353e-05 ... 8.05558353e-05\n",
            "  8.05558353e-05 8.05558353e-05]\n",
            " [4.62597690e-05 4.62597690e-05 4.62597690e-05 ... 4.62597690e-05\n",
            "  4.62597690e-05 4.62597690e-05]]\n",
            "P22222:\n",
            " [[5.82307263e-05 5.82307263e-05 5.82307263e-05 ... 5.82307263e-05\n",
            "  5.82307263e-05 5.82307263e-05]\n",
            " [3.92889384e-05 3.92889384e-05 3.92889384e-05 ... 3.92889384e-05\n",
            "  3.92889384e-05 3.92889384e-05]\n",
            " [5.96847995e-05 5.96847995e-05 5.96847995e-05 ... 5.96847995e-05\n",
            "  5.96847995e-05 5.96847995e-05]\n",
            " ...\n",
            " [8.05558353e-05 8.05558353e-05 8.05558353e-05 ... 8.05558353e-05\n",
            "  8.05558353e-05 8.05558353e-05]\n",
            " [8.05558353e-05 8.05558353e-05 8.05558353e-05 ... 8.05558353e-05\n",
            "  8.05558353e-05 8.05558353e-05]\n",
            " [6.08252743e-05 6.08252743e-05 6.08252743e-05 ... 6.08252743e-05\n",
            "  6.08252743e-05 6.08252743e-05]]\n",
            "iteration: 3\n",
            "iteration update 3\n",
            "P11111:\n",
            " [[4.47959869e-06 4.47959869e-06 4.47959869e-06 ... 4.47959869e-06\n",
            "  4.47959869e-06 4.47959869e-06]\n",
            " [2.83136578e-05 2.83136578e-05 2.83136578e-05 ... 2.83136578e-05\n",
            "  2.83136578e-05 2.83136578e-05]\n",
            " [5.36751069e-05 5.36751069e-05 5.36751069e-05 ... 5.36751069e-05\n",
            "  5.36751069e-05 5.36751069e-05]\n",
            " ...\n",
            " [8.05558353e-05 8.05558353e-05 8.05558353e-05 ... 8.05558353e-05\n",
            "  8.05558353e-05 8.05558353e-05]\n",
            " [8.05558353e-05 8.05558353e-05 8.05558353e-05 ... 8.05558353e-05\n",
            "  8.05558353e-05 8.05558353e-05]\n",
            " [4.55283864e-05 4.55283864e-05 4.55283864e-05 ... 4.55283864e-05\n",
            "  4.55283864e-05 4.55283864e-05]]\n",
            "P22222:\n",
            " [[5.66144282e-05 5.66144282e-05 5.66144282e-05 ... 5.66144282e-05\n",
            "  5.66144282e-05 5.66144282e-05]\n",
            " [3.90342246e-05 3.90342246e-05 3.90342246e-05 ... 3.90342246e-05\n",
            "  3.90342246e-05 3.90342246e-05]\n",
            " [6.17934804e-05 6.17934804e-05 6.17934804e-05 ... 6.17934804e-05\n",
            "  6.17934804e-05 6.17934804e-05]\n",
            " ...\n",
            " [8.05558353e-05 8.05558353e-05 8.05558353e-05 ... 8.05558353e-05\n",
            "  8.05558353e-05 8.05558353e-05]\n",
            " [8.05558353e-05 8.05558353e-05 8.05558353e-05 ... 8.05558353e-05\n",
            "  8.05558353e-05 8.05558353e-05]\n",
            " [6.06658730e-05 6.06658730e-05 6.06658730e-05 ... 6.06658730e-05\n",
            "  6.06658730e-05 6.06658730e-05]]\n",
            "iteration: 4\n",
            "iteration update 4\n",
            "P11111:\n",
            " [[4.47562073e-06 4.47562073e-06 4.47562073e-06 ... 4.47562073e-06\n",
            "  4.47562073e-06 4.47562073e-06]\n",
            " [2.81504313e-05 2.81504313e-05 2.81504313e-05 ... 2.81504313e-05\n",
            "  2.81504313e-05 2.81504313e-05]\n",
            " [5.37355182e-05 5.37355182e-05 5.37355182e-05 ... 5.37355182e-05\n",
            "  5.37355182e-05 5.37355182e-05]\n",
            " ...\n",
            " [8.05558353e-05 8.05558353e-05 8.05558353e-05 ... 8.05558353e-05\n",
            "  8.05558353e-05 8.05558353e-05]\n",
            " [8.05558353e-05 8.05558353e-05 8.05558353e-05 ... 8.05558353e-05\n",
            "  8.05558353e-05 8.05558353e-05]\n",
            " [4.52624185e-05 4.52624185e-05 4.52624185e-05 ... 4.52624185e-05\n",
            "  4.52624185e-05 4.52624185e-05]]\n",
            "P22222:\n",
            " [[5.56634638e-05 5.56634638e-05 5.56634638e-05 ... 5.56634638e-05\n",
            "  5.56634638e-05 5.56634638e-05]\n",
            " [3.81927495e-05 3.81927495e-05 3.81927495e-05 ... 3.81927495e-05\n",
            "  3.81927495e-05 3.81927495e-05]\n",
            " [6.06718233e-05 6.06718233e-05 6.06718233e-05 ... 6.06718233e-05\n",
            "  6.06718233e-05 6.06718233e-05]\n",
            " ...\n",
            " [8.05558353e-05 8.05558353e-05 8.05558353e-05 ... 8.05558353e-05\n",
            "  8.05558353e-05 8.05558353e-05]\n",
            " [8.05558353e-05 8.05558353e-05 8.05558353e-05 ... 8.05558353e-05\n",
            "  8.05558353e-05 8.05558353e-05]\n",
            " [5.95991012e-05 5.95991012e-05 5.95991012e-05 ... 5.95991012e-05\n",
            "  5.95991012e-05 5.95991012e-05]]\n",
            "iteration: 5\n",
            "iteration update 5\n",
            "P11111:\n",
            " [[4.47545881e-06 4.47545881e-06 4.47545881e-06 ... 4.47545881e-06\n",
            "  4.47545881e-06 4.47545881e-06]\n",
            " [2.79907543e-05 2.79907543e-05 2.79907543e-05 ... 2.79907543e-05\n",
            "  2.79907543e-05 2.79907543e-05]\n",
            " [5.37369255e-05 5.37369255e-05 5.37369255e-05 ... 5.37369255e-05\n",
            "  5.37369255e-05 5.37369255e-05]\n",
            " ...\n",
            " [8.05558353e-05 8.05558353e-05 8.05558353e-05 ... 8.05558353e-05\n",
            "  8.05558353e-05 8.05558353e-05]\n",
            " [8.05558353e-05 8.05558353e-05 8.05558353e-05 ... 8.05558353e-05\n",
            "  8.05558353e-05 8.05558353e-05]\n",
            " [4.51660375e-05 4.51660375e-05 4.51660375e-05 ... 4.51660375e-05\n",
            "  4.51660375e-05 4.51660375e-05]]\n",
            "P22222:\n",
            " [[5.49097116e-05 5.49097116e-05 5.49097116e-05 ... 5.49097116e-05\n",
            "  5.49097116e-05 5.49097116e-05]\n",
            " [3.78333966e-05 3.78333966e-05 3.78333966e-05 ... 3.78333966e-05\n",
            "  3.78333966e-05 3.78333966e-05]\n",
            " [5.98199879e-05 5.98199879e-05 5.98199879e-05 ... 5.98199879e-05\n",
            "  5.98199879e-05 5.98199879e-05]\n",
            " ...\n",
            " [8.05558353e-05 8.05558353e-05 8.05558353e-05 ... 8.05558353e-05\n",
            "  8.05558353e-05 8.05558353e-05]\n",
            " [8.05558353e-05 8.05558353e-05 8.05558353e-05 ... 8.05558353e-05\n",
            "  8.05558353e-05 8.05558353e-05]\n",
            " [5.89917298e-05 5.89917298e-05 5.89917298e-05 ... 5.89917298e-05\n",
            "  5.89917298e-05 5.89917298e-05]]\n",
            "iteration: 6\n",
            "iteration reset 6\n",
            "iteration: 7\n",
            "iteration update 7\n",
            "P11111:\n",
            " [[4.52805493e-06 4.52805493e-06 4.52805493e-06 ... 4.52805493e-06\n",
            "  4.52805493e-06 4.52805493e-06]\n",
            " [2.92591357e-05 2.92591357e-05 2.92591357e-05 ... 2.92591357e-05\n",
            "  2.92591357e-05 2.92591357e-05]\n",
            " [5.36489070e-05 5.36489070e-05 5.36489070e-05 ... 5.36489070e-05\n",
            "  5.36489070e-05 5.36489070e-05]\n",
            " ...\n",
            " [8.05558353e-05 8.05558353e-05 8.05558353e-05 ... 8.05558353e-05\n",
            "  8.05558353e-05 8.05558353e-05]\n",
            " [8.05558353e-05 8.05558353e-05 8.05558353e-05 ... 8.05558353e-05\n",
            "  8.05558353e-05 8.05558353e-05]\n",
            " [4.57225180e-05 4.57225180e-05 4.57225180e-05 ... 4.57225180e-05\n",
            "  4.57225180e-05 4.57225180e-05]]\n",
            "P22222:\n",
            " [[5.81136695e-05 5.81136695e-05 5.81136695e-05 ... 5.81136695e-05\n",
            "  5.81136695e-05 5.81136695e-05]\n",
            " [3.81232828e-05 3.81232828e-05 3.81232828e-05 ... 3.81232828e-05\n",
            "  3.81232828e-05 3.81232828e-05]\n",
            " [6.07692059e-05 6.07692059e-05 6.07692059e-05 ... 6.07692059e-05\n",
            "  6.07692059e-05 6.07692059e-05]\n",
            " ...\n",
            " [8.05558353e-05 8.05558353e-05 8.05558353e-05 ... 8.05558353e-05\n",
            "  8.05558353e-05 8.05558353e-05]\n",
            " [8.05558353e-05 8.05558353e-05 8.05558353e-05 ... 8.05558353e-05\n",
            "  8.05558353e-05 8.05558353e-05]\n",
            " [6.02855073e-05 6.02855073e-05 6.02855073e-05 ... 6.02855073e-05\n",
            "  6.02855073e-05 6.02855073e-05]]\n",
            "iteration: 8\n",
            "iteration update 8\n",
            "P11111:\n",
            " [[4.47603709e-06 4.47603709e-06 4.47603709e-06 ... 4.47603709e-06\n",
            "  4.47603709e-06 4.47603709e-06]\n",
            " [2.81776037e-05 2.81776037e-05 2.81776037e-05 ... 2.81776037e-05\n",
            "  2.81776037e-05 2.81776037e-05]\n",
            " [5.36989543e-05 5.36989543e-05 5.36989543e-05 ... 5.36989543e-05\n",
            "  5.36989543e-05 5.36989543e-05]\n",
            " ...\n",
            " [8.05558353e-05 8.05558353e-05 8.05558353e-05 ... 8.05558353e-05\n",
            "  8.05558353e-05 8.05558353e-05]\n",
            " [8.05558353e-05 8.05558353e-05 8.05558353e-05 ... 8.05558353e-05\n",
            "  8.05558353e-05 8.05558353e-05]\n",
            " [4.49519600e-05 4.49519600e-05 4.49519600e-05 ... 4.49519600e-05\n",
            "  4.49519600e-05 4.49519600e-05]]\n",
            "P22222:\n",
            " [[5.64439675e-05 5.64439675e-05 5.64439675e-05 ... 5.64439675e-05\n",
            "  5.64439675e-05 5.64439675e-05]\n",
            " [3.74635541e-05 3.74635541e-05 3.74635541e-05 ... 3.74635541e-05\n",
            "  3.74635541e-05 3.74635541e-05]\n",
            " [5.97081399e-05 5.97081399e-05 5.97081399e-05 ... 5.97081399e-05\n",
            "  5.97081399e-05 5.97081399e-05]\n",
            " ...\n",
            " [8.05558353e-05 8.05558353e-05 8.05558353e-05 ... 8.05558353e-05\n",
            "  8.05558353e-05 8.05558353e-05]\n",
            " [8.05558353e-05 8.05558353e-05 8.05558353e-05 ... 8.05558353e-05\n",
            "  8.05558353e-05 8.05558353e-05]\n",
            " [5.91412984e-05 5.91412984e-05 5.91412984e-05 ... 5.91412984e-05\n",
            "  5.91412984e-05 5.91412984e-05]]\n",
            "iteration: 9\n",
            "iteration update 9\n",
            "P11111:\n",
            " [[4.47540447e-06 4.47540447e-06 4.47540447e-06 ... 4.47540447e-06\n",
            "  4.47540447e-06 4.47540447e-06]\n",
            " [2.79022716e-05 2.79022716e-05 2.79022716e-05 ... 2.79022716e-05\n",
            "  2.79022716e-05 2.79022716e-05]\n",
            " [5.37185627e-05 5.37185627e-05 5.37185627e-05 ... 5.37185627e-05\n",
            "  5.37185627e-05 5.37185627e-05]\n",
            " ...\n",
            " [8.05558353e-05 8.05558353e-05 8.05558353e-05 ... 8.05558353e-05\n",
            "  8.05558353e-05 8.05558353e-05]\n",
            " [8.05558353e-05 8.05558353e-05 8.05558353e-05 ... 8.05558353e-05\n",
            "  8.05558353e-05 8.05558353e-05]\n",
            " [4.47255213e-05 4.47255213e-05 4.47255213e-05 ... 4.47255213e-05\n",
            "  4.47255213e-05 4.47255213e-05]]\n",
            "P22222:\n",
            " [[5.56325140e-05 5.56325140e-05 5.56325140e-05 ... 5.56325140e-05\n",
            "  5.56325140e-05 5.56325140e-05]\n",
            " [3.64862001e-05 3.64862001e-05 3.64862001e-05 ... 3.64862001e-05\n",
            "  3.64862001e-05 3.64862001e-05]\n",
            " [5.88828477e-05 5.88828477e-05 5.88828477e-05 ... 5.88828477e-05\n",
            "  5.88828477e-05 5.88828477e-05]\n",
            " ...\n",
            " [8.05558353e-05 8.05558353e-05 8.05558353e-05 ... 8.05558353e-05\n",
            "  8.05558353e-05 8.05558353e-05]\n",
            " [8.05558353e-05 8.05558353e-05 8.05558353e-05 ... 8.05558353e-05\n",
            "  8.05558353e-05 8.05558353e-05]\n",
            " [5.85541286e-05 5.85541286e-05 5.85541286e-05 ... 5.85541286e-05\n",
            "  5.85541286e-05 5.85541286e-05]]\n",
            "iteration: 10\n",
            "iteration update 10\n",
            "P11111:\n",
            " [[4.47538414e-06 4.47538414e-06 4.47538414e-06 ... 4.47538414e-06\n",
            "  4.47538414e-06 4.47538414e-06]\n",
            " [2.77664706e-05 2.77664706e-05 2.77664706e-05 ... 2.77664706e-05\n",
            "  2.77664706e-05 2.77664706e-05]\n",
            " [5.37181516e-05 5.37181516e-05 5.37181516e-05 ... 5.37181516e-05\n",
            "  5.37181516e-05 5.37181516e-05]\n",
            " ...\n",
            " [8.05558353e-05 8.05558353e-05 8.05558353e-05 ... 8.05558353e-05\n",
            "  8.05558353e-05 8.05558353e-05]\n",
            " [8.05558353e-05 8.05558353e-05 8.05558353e-05 ... 8.05558353e-05\n",
            "  8.05558353e-05 8.05558353e-05]\n",
            " [4.46646962e-05 4.46646962e-05 4.46646962e-05 ... 4.46646962e-05\n",
            "  4.46646962e-05 4.46646962e-05]]\n",
            "P22222:\n",
            " [[5.52354343e-05 5.52354343e-05 5.52354343e-05 ... 5.52354343e-05\n",
            "  5.52354343e-05 5.52354343e-05]\n",
            " [3.61788616e-05 3.61788616e-05 3.61788616e-05 ... 3.61788616e-05\n",
            "  3.61788616e-05 3.61788616e-05]\n",
            " [5.85152103e-05 5.85152103e-05 5.85152103e-05 ... 5.85152103e-05\n",
            "  5.85152103e-05 5.85152103e-05]\n",
            " ...\n",
            " [8.05558353e-05 8.05558353e-05 8.05558353e-05 ... 8.05558353e-05\n",
            "  8.05558353e-05 8.05558353e-05]\n",
            " [8.05558353e-05 8.05558353e-05 8.05558353e-05 ... 8.05558353e-05\n",
            "  8.05558353e-05 8.05558353e-05]\n",
            " [5.82675842e-05 5.82675842e-05 5.82675842e-05 ... 5.82675842e-05\n",
            "  5.82675842e-05 5.82675842e-05]]\n",
            "emperical_cross_entropy: 285.8699161173558\n"
          ]
        }
      ],
      "source": [
        "from scipy.sparse import csr_matrix, find\n",
        "import math\n",
        "import gc\n",
        "\n",
        "alpha = 0.8\n",
        "learning_rate = .1  # Learning rate for updates\n",
        "max_iterations = 11\n",
        "rep_lambda = 1\n",
        "\n",
        "\n",
        "###### initial\n",
        "CT = C_train_csr_slice2\n",
        "# print('CT:\\n', CT.todense())\n",
        "\n",
        "# Example usage\n",
        "P1 = SmartInitialization_p_sparse(C_train_csr, vocab_size)\n",
        "# print('P1_normalized_sparse:\\n', P1)\n",
        "# print('P1_normalized_dense:\\n', P1.todense())\n",
        "\n",
        "P2 = SmartInitialization_p_sparse(C_train_csr_slice2, vocab_size)\n",
        "# print('P2_normalized_sparse:\\n', P2)\n",
        "# print('P2_normalized_sparse:\\n', P2.todense())\n",
        "\n",
        "# lambda_matrix = np.array(\n",
        "#     [[1, 0, 0.3],  # Flattened representation of first \"row\" in 3D space\n",
        "#     [0, 5, 4],  # Flattened representation of second \"row\" in 3D space\n",
        "#     [0.1, 3, 0]]  # Flattened representation of third \"row\" in 3D space\n",
        "#  , dtype = np.float32)\n",
        "\n",
        "# Set seed for reproducibility\n",
        "np.random.seed(42)\n",
        "lambda_matrix = np.random.rand(vocab_size, vocab_size)\n",
        "lambda_matrix = csr_matrix(lambda_matrix)\n",
        "print('lambda_matrix:\\n',lambda_matrix.todense())\n",
        "\n",
        "\n",
        "# C_test = np.array([\n",
        "#     [[1, 5, 0],  # Flattened representation of first \"row\" in 3D space\n",
        "#     [0, 0, 1],  # Flattened representation of second \"row\" in 3D space\n",
        "#     [0, 5, 0]]  # Flattened representation of third \"row\" in 3D space\n",
        "#     ,\n",
        "#     [[2, 3, 0],  # Flattened representation of first \"row\" in 3D space\n",
        "#     [0, 5, 0],  # Flattened representation of second \"row\" in 3D space\n",
        "#     [7, 1, 0]]  # Flattened representation of third \"row\" in 3D space\n",
        "#     ,\n",
        "#     [[2, 0, 0],  # Flattened representation of first \"row\" in 3D space\n",
        "#     [1, 2, 9],  # Flattened representation of second \"row\" in 3D space\n",
        "#     [0, 3, 0]]  # Flattened representation of third \"row\" in 3D space\n",
        "\n",
        "# , ], dtype = np.float32)\n",
        "\n",
        "\n",
        "# # Flatten C_test by combining the last two dimensions\n",
        "# C_test_flattened = C_test.reshape(C_test.shape[0], -1)\n",
        "\n",
        "# # Example of how the flattened structure looks\n",
        "# # print(\"Flattened C_test:\")\n",
        "# # print(C_test_flattened)\n",
        "\n",
        "# # Create a sparse CSR matrix from the flattened structure\n",
        "# # First, find the indices of non-zero elements\n",
        "# rows, cols = np.nonzero(C_test_flattened)\n",
        "# values = C_test_flattened[rows, cols]\n",
        "\n",
        "# # Now, create the CSR matrix\n",
        "# C_test = csr_matrix((values, (rows, cols)), shape=C_test_flattened.shape)\n",
        "# print('C_test_sparse in dense form:\\n', C_test.todense())\n",
        "\n",
        "# d_i_sparse1 = np.array(\n",
        "#  [[2.],\n",
        "#  [1.],\n",
        "#  [5.]])\n",
        "# n_i = np.array(\n",
        "#  [[2.],\n",
        "#  [10.],\n",
        "#  [20.]])\n",
        "# lambda_matrix = np.random.rand(vocab_size, vocab_size)\n",
        "# # print(\"initial_lambda_matrix:\\n\",  lambda_matrix)\n",
        "# # P1 = SmartInitialization_p1(C_train)\n",
        "# # P2 = SmartInitialization_p2(C_train)\n",
        "# P1 = np.random.rand(vocab_size, vocab_size)\n",
        "# P2 = np.random.rand(vocab_size, vocab_size)\n",
        "\n",
        "# def MultiplicativeUpdate(C, lambda_matrix, P1, P2, max_iterations, rep_lambda, learning_rate = 1):\n",
        "\n",
        "\n",
        "def get_empirical_cross_entropy_sparse(C, P1_csr, P2_csr, lambda_csr, N):\n",
        "    empirical_cross_entropy = 0\n",
        "    count = 0\n",
        "    C_nonzero = find(C)\n",
        "    # Assuming C and P_hat can be aligned or P_hat's necessary values can be efficiently computed:\n",
        "    for i, idx, c_val in zip(*C_nonzero):\n",
        "        i2 = i\n",
        "        i1, j = get_original_indices(idx, vocab_size)\n",
        "        # Extract corresponding non-zero values from P1, P2, and lambda_matrix\n",
        "        # Directly access values from CSR matrices\n",
        "        p1_val = P1_csr[i1, j] if P1_csr[i1, j] != 0 else 0\n",
        "        p2_val = P2_csr[i2, j] if P2_csr[i2, j] != 0 else 0\n",
        "        lambda_val = lambda_csr[i1, i2] if lambda_csr[i1, i2] != 0 else 0\n",
        "        # Compute the denominator for the update formula\n",
        "        p_val = lambda_val * p1_val + (1 - lambda_val) * p2_val\n",
        "        count += c_val\n",
        "\n",
        "        if p_val > 0:\n",
        "            empirical_cross_entropy -= c_val * math.log2(p_val)\n",
        "        else:\n",
        "          empirical_cross_entropy -= 0  # Or some default value, or you can raise an error\n",
        "    return math.pow(2, empirical_cross_entropy / N), count\n",
        "\n",
        "\n",
        "for iteration in range(max_iterations):\n",
        "\n",
        "    print('iteration:', iteration)\n",
        "\n",
        "    # Calculate the denominator: sum of C across the last axis (j)\n",
        "    CT = C_train_csr_slice2\n",
        "    # print('CT:\\n', CT.todense())\n",
        "\n",
        "    # # Prevent division by zero by setting zero denominators to 1 (or add a small epsilon)\n",
        "    # denominator_lambda[denominator_lambda == 0] = 1e-10\n",
        "\n",
        "    lambda_updated = update_lambda_sparse(CT, P1, P2, lambda_matrix, vocab_size)\n",
        "    # Compute the scaled difference\n",
        "    scaled_diff = (lambda_updated - lambda_matrix).multiply(learning_rate)\n",
        "    # Compute the new lambda_matrix\n",
        "    lambda_matrix = lambda_matrix + scaled_diff\n",
        "    # print('lambda_matrix111:\\n',lambda_matrix.todense())\n",
        "\n",
        "    # # Optional: Convert back to CSR if you had converted it before\n",
        "    # lambda_matrix = csr_matrix(lambda_matrix_new)\n",
        "\n",
        "\n",
        "    multipliers = [5,10]  # You can extend this list to include more multipliers\n",
        "    # multipliers = list(range(1,10))\n",
        "    if iteration == 0 or iteration > rep_lambda:    #only update lambda and P1 and P2 in first iteration and after that iterates only lambda and then after rep_lambda iterates P1 and P2 as well\n",
        "\n",
        "      # if iteration == rep_lambda+1 or iteration == 5*rep_lambda+1 or iteration == 10*rep_lambda+1 or iteration == 15*rep_lambda+1:    #not work\n",
        "      # if iteration == rep_lambda+1 or iteration == 5*rep_lambda+1 or iteration == 10*rep_lambda+1:\n",
        "      # if iteration > rep_lambda and (iteration - 1) % rep_lambda == 0:   #the condition will be true for iterations like rep_lambda + 1, 2*rep_lambda + 1, 3*rep_lambda + 1, and so on.\n",
        "\n",
        "      if iteration == 0:\n",
        "        ## reseting P1 and P2 after rep_lamda so it measns that with smart initialization of P1 and P2 first update lambda with rep_lambda iterations and then reset P1 and P2 since they are good\n",
        "        P1 = SmartInitialization_p_sparse(C_train_csr, vocab_size)\n",
        "        P2 = SmartInitialization_p_sparse(C_train_csr_slice2, vocab_size)\n",
        "        print('iteration initial',iteration)\n",
        "\n",
        "      if any(iteration == m * rep_lambda + 1 for m in multipliers):\n",
        "        P1 = SmartInitialization_p_sparse(C_train_csr, vocab_size)\n",
        "        P2 = SmartInitialization_p_sparse(C_train_csr_slice2, vocab_size)\n",
        "\n",
        "        print('iteration reset',iteration)\n",
        "\n",
        "      elif iteration != 0:\n",
        "\n",
        "        print('iteration update',iteration)\n",
        "\n",
        "        # Update P1\n",
        "        numerator_P1 = update_p1_sparse(CT, P1, P2, lambda_matrix, vocab_size)\n",
        "        A_slice1, n_i = Smoothing_P_sparse(numerator_P1, alpha=0.8)\n",
        "        d_i_sparse1 = DistinctTokenSparse(numerator_P1)\n",
        "        total_missing_mass1 = alpha * d_i_sparse1/n_i\n",
        "\n",
        "        # Convert missing_mass_per_vocab_sparse to a format that can be added to A_slice1\n",
        "        # We're creating a dense array of the same shape as A_slice1 with each row having the same value\n",
        "        missing_mass_per_vocab = np.repeat(total_missing_mass1 / vocab_size, vocab_size, axis=1)\n",
        "        # Convert the missing_mass_per_vocab to a sparse matrix\n",
        "        missing_mass_per_vocab_sparse = csr_matrix(missing_mass_per_vocab)\n",
        "\n",
        "        # Add the missing_mass_per_vocab_sparse to A_slice1 element-wise (smoothing method to add absolute discount to each element and remove zero element)\n",
        "        P1 = A_slice1 + missing_mass_per_vocab_sparse\n",
        "        print('P11111:\\n',P1.todense())\n",
        "\n",
        "\n",
        "        # # Update P2\n",
        "        numerator_P2 = update_p2_sparse(CT, P1, P2, lambda_matrix, vocab_size)\n",
        "        A_slice2, n_i = Smoothing_P_sparse(numerator_P2, alpha=0.8)\n",
        "        d_i_sparse2 = DistinctTokenSparse(numerator_P2)\n",
        "        total_missing_mass2 = alpha * d_i_sparse2/n_i\n",
        "        # Convert missing_mass_per_vocab_sparse to a format that can be added to A_slice1\n",
        "        # We're creating a dense array of the same shape as A_slice1 with each row having the same value\n",
        "        missing_mass_per_vocab2 = np.repeat(total_missing_mass2 / vocab_size, vocab_size, axis=1)\n",
        "        # Convert the missing_mass_per_vocab to a sparse matrix\n",
        "        missing_mass_per_vocab_sparse2 = csr_matrix(missing_mass_per_vocab2)\n",
        "        P2 = A_slice2 + missing_mass_per_vocab_sparse2\n",
        "        print('P22222:\\n',P2.todense())\n",
        "        # P2 = numerator_P2 / np.sum(numerator_P2, axis=1, keepdims=True)\n",
        "        # P2 /= P2.sum(axis=1, keepdims=True)\n",
        "    # if iteration % 2 == 0:\n",
        "    #   P_hat = find_P_hat(lambda_matrix, P1, P2)\n",
        "    #   # print('P_hat:', P_hat)\n",
        "    #   N = test_size\n",
        "    #   empirical_cross_entropy = get_empirical_cross_entropy_sparse(C_test, P_hat, N)\n",
        "    #   print('empirical_cross_entropy:', empirical_cross_entropy)\n",
        "        # Clear memory explicitly\n",
        "        gc.collect()\n",
        "\n",
        "\n",
        "# print('lambda_matrix_updated:\\n',lambda_matrix)\n",
        "\n",
        "# print('P1_updated:\\n', P1)\n",
        "# print('P1_updated_dense:\\n', P1.todense())\n",
        "\n",
        "# print('P2_updated:\\n', P2)\n",
        "# print('P2_updated_dense:\\n', P2.todense())\n",
        "\n",
        "# def find_P_hat(lambda_matrix, P1, P2):\n",
        "# # Initialize the 3-dimensional tensor P_hat\n",
        "#   P_hat = np.zeros((vocab_size, vocab_size, vocab_size))  # Tensor for i2, i1, j\n",
        "\n",
        "#   # Populate the tensor P using the provided equation\n",
        "#   for i2 in range(vocab_size):\n",
        "#       for i1 in range(vocab_size):\n",
        "#           for j in range(vocab_size):\n",
        "#               P_hat[i2, i1, j] = lambda_matrix[i1, i2] * P1[i1, j] + (1 - lambda_matrix[i1, i2]) * P2[i2, j]\n",
        "\n",
        "#   # print(\"Tensor P_hat:\\n\", P_hat)\n",
        "#   return P_hat\n",
        "\n",
        "# P_hat = find_P_hat(lambda_matrix, P1, P2)\n",
        "\n",
        "\n",
        "\n",
        "# # Assuming lambda_matrix, P1, P2 are defined sparse matrices\n",
        "# P_hat = find_P_hat(lambda_matrix, P1, P2)\n",
        "\n",
        "# print('-----------------------------------------')\n",
        "\n",
        "N = test_size\n",
        "empirical_cross_entropy, count = get_empirical_cross_entropy_sparse(C_test, P1, P2, lambda_matrix, N)\n",
        "print('empirical_cross_entropy:', empirical_cross_entropy)\n",
        "# print('count:', count)\n",
        "\n",
        "\n",
        "# empirical_cross_entropy: 276.6979117176241\n",
        "# emp2 892.2182737491097\n",
        "# count: 67598\n",
        "# N: 81673\n",
        "# test_size: 81673"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1xezV0L1oRnc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1xezV0L1oRnc",
        "outputId": "3305caec-f8ff-4bb4-a301-de47c5040bc2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "lambda_matrix_target:\n",
            " [[1.  0.  0.3]\n",
            " [0.  5.  4. ]\n",
            " [0.1 3.  0. ]]\n",
            "target_P1:\n",
            " [[0.7 0.  0.3]\n",
            " [0.1 0.5 0.4]\n",
            " [1.  0.  0. ]]\n",
            "target_P2:\n",
            " [[0.1 0.6 0.3]\n",
            " [0.  1.  0. ]\n",
            " [0.1 0.5 0.4]]\n",
            "Tensor P:\n",
            " [[[ 0.69999999  0.          0.30000001]\n",
            "  [ 0.1         0.60000002  0.30000001]\n",
            "  [ 0.19        0.54000002  0.27000001]]\n",
            "\n",
            " [[ 0.          1.          0.        ]\n",
            "  [ 0.5        -1.5         2.        ]\n",
            "  [ 3.         -2.          0.        ]]\n",
            "\n",
            " [[ 0.28000001  0.34999999  0.37      ]\n",
            "  [ 0.1         0.5         0.40000001]\n",
            "  [ 0.1         0.5         0.40000001]]]\n"
          ]
        }
      ],
      "source": [
        "# Final version how to create P with for loop\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "lambda_matrix_target = np.array(\n",
        "    [[1, 0, 0.3],  # Flattened representation of first \"row\" in 3D space\n",
        "    [0, 5, 4],  # Flattened representation of second \"row\" in 3D space\n",
        "    [0.1, 3, 0]]  # Flattened representation of third \"row\" in 3D space\n",
        " , dtype = np.float32)\n",
        "print('lambda_matrix_target:\\n',lambda_matrix_target)\n",
        "\n",
        "# Check that the rows sum to 1\n",
        "P1_target = np.array(\n",
        "    [[0.7, 0, 0.3],  # Flattened representation of first \"row\" in 3D space\n",
        "    [0.1, .5, .4],  # Flattened representation of second \"row\" in 3D space\n",
        "    [1, 0, 0]]  # Flattened representation of third \"row\" in 3D space\n",
        " , dtype = np.float32)\n",
        "print(\"target_P1:\\n\", P1_target)\n",
        "\n",
        "P2_target = np.array(\n",
        "    [[.1, 0.6, 0.3],  # Flattened representation of first \"row\" in 3D space\n",
        "    [0, 1, 0],  # Flattened representation of second \"row\" in 3D space\n",
        "    [0.1, 0.5, 0.4]]  # Flattened representation of third \"row\" in 3D space\n",
        " , dtype = np.float32)\n",
        "print(\"target_P2:\\n\", P2_target)\n",
        "\n",
        "# Initialize the 3-dimensional tensor P\n",
        "P = np.zeros((3, 3, 3))  # Tensor for i2, i1, j\n",
        "\n",
        "# Populate the tensor P using the provided equation\n",
        "for i2 in range(3):\n",
        "    for i1 in range(3):\n",
        "        for j in range(3):\n",
        "            P[i2, i1, j] = lambda_matrix_target[i1, i2] * P1_target[i1, j] + (1 - lambda_matrix_target[i1, i2]) * P2_target[i2, j]\n",
        "\n",
        "print(\"Tensor P:\\n\", P)\n",
        "vocab_size = 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jMIUjHIdCeuW",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jMIUjHIdCeuW",
        "outputId": "b28180ab-072d-469c-ce05-3ab54f9010ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "C_train =\n",
            " [[[ 95.   0.  37.]\n",
            "  [ 14.  61.  34.]\n",
            "  [ 23.  68.  31.]]\n",
            "\n",
            " [[  0.  93.   0.]\n",
            "  [ 13.  65.  48.]\n",
            "  [108.   0.   0.]]\n",
            "\n",
            " [[ 31.  42.  33.]\n",
            "  [  7.  45.  42.]\n",
            "  [ 17.  50.  43.]]]\n"
          ]
        }
      ],
      "source": [
        "# Final version\n",
        "# Create test dataset from an empirical distribution\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "# Assuming P1, P2, target_lambda_matrix are already defined and appropriately normalized\n",
        "\n",
        "# Set seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "\n",
        "def CreateDataset(dataset_size, lambda_matrix, P1, P2):\n",
        "\n",
        "  C = np.zeros((3, 3, 3))\n",
        "  for k in range(dataset_size):\n",
        "      # Randomly sample i1 and i2\n",
        "      i1, i2 = np.random.choice([0, 1, 2], 2)\n",
        "\n",
        "      # Decide to choose from P1 or P2 based on lambda\n",
        "      coin_flip = np.random.rand() < lambda_matrix[i1, i2]\n",
        "\n",
        "      # Sample j based on the outcome of the coin flip\n",
        "      if coin_flip:  # If true, choose from P1 using row i1\n",
        "          chosen_row = P1[i1]\n",
        "      else:  # Else, choose from P2 using row i2\n",
        "          chosen_row = P2[i2]\n",
        "\n",
        "      # Sample j from the chosen matrix row\n",
        "      j = np.random.choice([0, 1, 2], p=chosen_row)\n",
        "\n",
        "      # increment a count or sum probabilities in C[i2, i1, j]\n",
        "      C[i2, i1, j] += 1\n",
        "\n",
        "  return C\n",
        "\n",
        "\n",
        "# Generate train data\n",
        "train_size = 1000  # Number of samples in the test dataset\n",
        "C_train = CreateDataset(train_size, lambda_matrix_target, P1_target, P2_target)\n",
        "\n",
        "print('C_train =\\n', C_train)\n",
        "\n",
        "# Generate test data\n",
        "test_size = 100  # Number of samples in the test dataset\n",
        "C_test = CreateDataset(test_size, lambda_matrix_target, P1_target, P2_target)\n",
        "\n",
        "# print('C_test =\\n', C_test)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nbsiYgnGq-sZ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nbsiYgnGq-sZ",
        "outputId": "964b05c6-90d1-4a02-eb23-16ec89c35434"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "P1:\n",
            " [[0.38066465 0.40785498 0.21148036]\n",
            " [0.10334347 0.51975684 0.3768997 ]\n",
            " [0.43529412 0.34705882 0.21764706]]\n",
            "P2:\n",
            " [[0.36363636 0.3553719  0.28099174]\n",
            " [0.37003058 0.48318043 0.14678899]\n",
            " [0.17741935 0.44193548 0.38064516]]\n",
            "lambda_matrix:\n",
            " [[0.37454012 0.95071431 0.73199394]\n",
            " [0.59865848 0.15601864 0.15599452]\n",
            " [0.05808361 0.86617615 0.60111501]]\n",
            "iteration initial 0\n",
            "iteration reset 6\n",
            "iteration update 7\n",
            "iteration update 8\n",
            "iteration update 9\n",
            "Iteration 10, Max change: 4.843934166494029\n",
            "Updated lambda_matrix:\n",
            " [[0.37451426 0.95070839 0.73198098]\n",
            " [0.59868978 0.15606583 0.15599935]\n",
            " [0.05807888 0.8662101  0.60103416]]\n",
            "Iteration 10, Max change: 0.6092795785695052\n",
            "Updated P1:\n",
            " [[0.28813304 0.60927958 0.10258738]\n",
            " [0.02696165 0.62771785 0.3453205 ]\n",
            " [0.81882175 0.11793447 0.06324378]]\n",
            "Iteration 10, Max change: 0.49596619704929923\n",
            "Updated P2:\n",
            " [[0.38906087 0.32402204 0.28691709]\n",
            " [0.13720668 0.5040338  0.35875952]\n",
            " [0.05136919 0.43528249 0.51334832]]\n",
            "Tensor P_hat:\n",
            " [[[0.35126196 0.43085506 0.21788298]\n",
            "  [0.17227577 0.50584162 0.32188262]\n",
            "  [0.4140209  0.3120527  0.27392639]]\n",
            "\n",
            " [[0.28069364 0.60409184 0.11521452]\n",
            "  [0.1200012  0.52333666 0.35666214]\n",
            "  [0.72762854 0.16959066 0.1027808 ]]\n",
            "\n",
            " [[0.22467583 0.56264505 0.21267912]\n",
            "  [0.04756163 0.46530228 0.48713609]\n",
            "  [0.51263439 0.24454549 0.24282012]]]\n",
            "-----------------------------------------\n",
            "emperical_cross_entropy: 2.55794347165544\n"
          ]
        }
      ],
      "source": [
        "from scipy.sparse import csr_matrix, find\n",
        "import math\n",
        "import gc\n",
        "\n",
        "alpha = 0.8\n",
        "learning_rate = .0001  # Learning rate for updates\n",
        "max_iterations = 10\n",
        "rep_lambda = 5\n",
        "\n",
        "\n",
        "def SmartInitialization_p1(C):\n",
        "  ########## smart initialization for P1\n",
        "  P1 = np.sum(C, axis=0)\n",
        "  # Ensure P1 and P2 rows sum to 1 for probability matrices\n",
        "  P1 /= P1.sum(axis=1, keepdims=True)\n",
        "  # print(\"initial_P1:\\n\", P1)\n",
        "  return P1\n",
        "\n",
        "\n",
        "def SmartInitialization_p2(C):\n",
        "  ########## smart initialization for P2\n",
        "  P2 = np.sum(C, axis=1)\n",
        "  # Ensure P1 and P2 rows sum to 1 for probability matrices\n",
        "  P2 /= P2.sum(axis=1, keepdims=True)\n",
        "  # print(\"initial_P2:\\n\", P2)\n",
        "  return P2\n",
        "\n",
        "\n",
        "P1 = SmartInitialization_p1(C_train)\n",
        "P2 = SmartInitialization_p2(C_train)\n",
        "\n",
        "P1 = csr_matrix(P1)\n",
        "P2 = csr_matrix(P2)\n",
        "print('P1:\\n',P1.todense())\n",
        "print('P2:\\n',P2.todense())\n",
        "\n",
        "# Set seed for reproducibility\n",
        "random_seed = 42\n",
        "random.seed(random_seed)  # Python's built-in random module\n",
        "np.random.seed(random_seed)  # NumPy\n",
        "lambda_matrix = np.random.rand(3, 3)\n",
        "\n",
        "lambda_matrix = csr_matrix(lambda_matrix)\n",
        "print('lambda_matrix:\\n',lambda_matrix.todense())\n",
        "\n",
        "\n",
        "###### initial\n",
        "CT = np.transpose(C_train, axes=(1,0,2))\n",
        "# print('CT:\\n', CT)\n",
        "CT = CT.reshape(3,9)\n",
        "# print('CT_reshape:\\n', CT)\n",
        "CT = csr_matrix(CT)\n",
        "# CT_nonzero = find(CT)\n",
        "# print('CT_nonzero:\\n', CT_nonzero)\n",
        "\n",
        "def get_empirical_cross_entropy_sparse(C, P1_csr, P2_csr, lambda_csr, N):\n",
        "    empirical_cross_entropy = 0\n",
        "    count = 0\n",
        "    C_nonzero = find(C)\n",
        "    # Assuming C and P_hat can be aligned or P_hat's necessary values can be efficiently computed:\n",
        "    for i, idx, c_val in zip(*C_nonzero):\n",
        "        i2 = i\n",
        "        i1, j = get_original_indices(idx, vocab_size)\n",
        "        # Extract corresponding non-zero values from P1, P2, and lambda_matrix\n",
        "        # Directly access values from CSR matrices\n",
        "        p1_val = P1_csr[i1, j] if P1_csr[i1, j] != 0 else 0\n",
        "        p2_val = P2_csr[i2, j] if P2_csr[i2, j] != 0 else 0\n",
        "        lambda_val = lambda_csr[i1, i2] if lambda_csr[i1, i2] != 0 else 0\n",
        "        # Compute the denominator for the update formula\n",
        "        p_val = lambda_val * p1_val + (1 - lambda_val) * p2_val\n",
        "        count += c_val\n",
        "\n",
        "        if p_val > 0:\n",
        "            empirical_cross_entropy -= c_val * math.log2(p_val)\n",
        "        else:\n",
        "          empirical_cross_entropy -= 0  # Or some default value, or you can raise an error\n",
        "    return math.pow(2, empirical_cross_entropy / N), count\n",
        "\n",
        "\n",
        "for iteration in range(max_iterations):\n",
        "\n",
        "    # print('iteration:', iteration)\n",
        "\n",
        "    # Calculate the denominator: sum of C across the last axis (j)\n",
        "    # CT = C_train_csr_slice2\n",
        "    CT = CT\n",
        "    # print('CT:\\n', CT.todense())\n",
        "\n",
        "    # # Prevent division by zero by setting zero denominators to 1 (or add a small epsilon)\n",
        "    # denominator_lambda[denominator_lambda == 0] = 1e-10\n",
        "\n",
        "    lambda_updated = update_lambda_sparse(CT, P1, P2, lambda_matrix, vocab_size)\n",
        "    # Compute the scaled difference\n",
        "    scaled_diff = (lambda_updated - lambda_matrix).multiply(learning_rate)\n",
        "    # Compute the new lambda_matrix\n",
        "    lambda_matrix = lambda_matrix + scaled_diff\n",
        "    # # Optional: Convert back to CSR if you had converted it before\n",
        "    # lambda_matrix = csr_matrix(lambda_matrix_new)\n",
        "\n",
        "\n",
        "    # lambda_expanded = lambda_matrix[:, :, np.newaxis]\n",
        "\n",
        "    multipliers = [1]  # You can extend this list to include more multipliers\n",
        "    # multipliers = list(range(1,10))\n",
        "    if iteration == 0 or iteration > rep_lambda:    #only update lambda and P1 and P2 in first iteration and after that iterates only lambda and then after rep_lambda iterates P1 and P2 as well\n",
        "\n",
        "      # if iteration == rep_lambda+1 or iteration == 5*rep_lambda+1 or iteration == 10*rep_lambda+1 or iteration == 15*rep_lambda+1:    #not work\n",
        "      # if iteration == rep_lambda+1 or iteration == 5*rep_lambda+1 or iteration == 10*rep_lambda+1:\n",
        "      # if iteration > rep_lambda and (iteration - 1) % rep_lambda == 0:   #the condition will be true for iterations like rep_lambda + 1, 2*rep_lambda + 1, 3*rep_lambda + 1, and so on.\n",
        "\n",
        "      if iteration == 0:\n",
        "        ## reseting P1 and P2 after rep_lamda so it measns that with smart initialization of P1 and P2 first update lambda with rep_lambda iterations and then reset P1 and P2 since they are good\n",
        "        # P1 = SmartInitialization_p_sparse(C_train_csr, vocab_size)\n",
        "        # P2 = SmartInitialization_p_sparse(C_train_csr_slice2, vocab_size)\n",
        "        P1 = SmartInitialization_p1(C_train)\n",
        "        P2 = SmartInitialization_p2(C_train)\n",
        "\n",
        "        P1 = csr_matrix(P1)\n",
        "        P2 = csr_matrix(P2)\n",
        "        print('iteration initial',iteration)\n",
        "\n",
        "      if any(iteration == m * rep_lambda + 1 for m in multipliers):\n",
        "        P1 = SmartInitialization_p1(C_train)\n",
        "\n",
        "        P2 = SmartInitialization_p2(C_train)\n",
        "        print('iteration reset',iteration)\n",
        "\n",
        "      elif iteration != 0:\n",
        "\n",
        "        print('iteration update',iteration)\n",
        "\n",
        "        # Update P1\n",
        "        numerator_P1 = update_p1_sparse(CT, P1, P2, lambda_matrix, vocab_size)\n",
        "        A_slice1, n_i = Smoothing_P_sparse(numerator_P1, alpha=0.8)\n",
        "        d_i_sparse1 = DistinctTokenSparse(numerator_P1)\n",
        "        total_missing_mass1 = alpha * d_i_sparse1/n_i\n",
        "\n",
        "        # Convert missing_mass_per_vocab_sparse to a format that can be added to A_slice1\n",
        "        # We're creating a dense array of the same shape as A_slice1 with each row having the same value\n",
        "        missing_mass_per_vocab = np.repeat(total_missing_mass1 / vocab_size, vocab_size, axis=1)\n",
        "        # Convert the missing_mass_per_vocab to a sparse matrix\n",
        "        missing_mass_per_vocab_sparse = csr_matrix(missing_mass_per_vocab)\n",
        "\n",
        "        # Add the missing_mass_per_vocab_sparse to A_slice1 element-wise (smoothing method to add absolute discount to each element and remove zero element)\n",
        "        P1 = A_slice1 + missing_mass_per_vocab_sparse\n",
        "        # print('P11111:\\n',P1.todense())\n",
        "\n",
        "\n",
        "        # # Update P2\n",
        "        numerator_P2 = update_p2_sparse(CT, P1, P2, lambda_matrix, vocab_size)\n",
        "        A_slice2, n_i = Smoothing_P_sparse(numerator_P2, alpha=0.8)\n",
        "        d_i_sparse2 = DistinctTokenSparse(numerator_P2)\n",
        "        total_missing_mass2 = alpha * d_i_sparse2/n_i\n",
        "        # Convert missing_mass_per_vocab_sparse to a format that can be added to A_slice1\n",
        "        # We're creating a dense array of the same shape as A_slice1 with each row having the same value\n",
        "        missing_mass_per_vocab2 = np.repeat(total_missing_mass2 / vocab_size, vocab_size, axis=1)\n",
        "        # Convert the missing_mass_per_vocab to a sparse matrix\n",
        "        missing_mass_per_vocab_sparse2 = csr_matrix(missing_mass_per_vocab2)\n",
        "        P2 = A_slice2 + missing_mass_per_vocab_sparse2\n",
        "        # print('P22222:\\n',P2.todense())\n",
        "        # P2 = numerator_P2 / np.sum(numerator_P2, axis=1, keepdims=True)\n",
        "        # P2 /= P2.sum(axis=1, keepdims=True)\n",
        "    # if iteration % 2 == 0:\n",
        "    #   P_hat = find_P_hat(lambda_matrix, P1, P2)\n",
        "    #   # print('P_hat:', P_hat)\n",
        "    #   N = test_size\n",
        "    #   empirical_cross_entropy = get_empirical_cross_entropy_sparse(C_test, P_hat, N)\n",
        "    #   print('empirical_cross_entropy:', empirical_cross_entropy)\n",
        "        # Clear memory explicitly\n",
        "        gc.collect()\n",
        "\n",
        "\n",
        "# print('lambda_matrix_updated:\\n',lambda_matrix)\n",
        "\n",
        "# print('P1_updated:\\n', P1)\n",
        "# print('P1_updated_dense:\\n', P1.todense())\n",
        "\n",
        "# print('P2_updated:\\n', P2)\n",
        "# print('P2_updated_dense:\\n', P2.todense())\n",
        "\n",
        "\n",
        "# P1 = SmartInitialization_p1(C_train)\n",
        "# P2 = SmartInitialization_p2(C_train)\n",
        "# lambda_matrix, P1, P2 = MultiplicativeUpdate(C_train, lambda_matrix, P1, P2, max_iterations, rep_lambda, learning_rate)\n",
        "\n",
        "# Print the updated lambda_matrix\n",
        "print(f\"Iteration {max_iterations}, Max change: {np.max(np.abs(lambda_matrix - lambda_matrix_target))}\")\n",
        "print(\"Updated lambda_matrix:\\n\", lambda_matrix.todense())\n",
        "print(f\"Iteration {max_iterations}, Max change: {np.max(np.abs(P1 - P1_target))}\")\n",
        "# Print out the final lambda matrix\n",
        "print(\"Updated P1:\\n\", P1.todense())\n",
        "print(f\"Iteration {max_iterations}, Max change: {np.max(np.abs(P2 - P2_target))}\")\n",
        "print(\"Updated P2:\\n\", P2.todense())\n",
        "\n",
        "\n",
        "# P_hat is a learned dataset\n",
        "# Initialize the 3-dimensional tensor P_hat\n",
        "P_hat = np.zeros((3, 3, 3))  # Tensor for i2, i1, j\n",
        "\n",
        "# Populate the tensor P using the provided equation\n",
        "for i2 in range(3):\n",
        "    for i1 in range(3):\n",
        "        for j in range(3):\n",
        "            P_hat[i2, i1, j] = lambda_matrix[i1, i2] * P1[i1, j] + (1 - lambda_matrix[i1, i2]) * P2[i2, j]\n",
        "\n",
        "print(\"Tensor P_hat:\\n\", P_hat)\n",
        "print('-----------------------------------------')\n",
        "\n",
        "\n",
        "N = test_size\n",
        "C_test = C_test.reshape(3,9)\n",
        "C_test = csr_matrix(C_test)\n",
        "empirical_cross_entropy, count = get_empirical_cross_entropy_sparse(C_test, P1, P2, lambda_matrix, N)\n",
        "print('empirical_cross_entropy:', empirical_cross_entropy)\n",
        "# print('count:', count)\n",
        "\n",
        "\n",
        "# empirical_cross_entropy: 276.6979117176241\n",
        "# emp2 892.2182737491097\n",
        "# count: 67598\n",
        "# N: 81673\n",
        "# test_size: 81673\n",
        "\n",
        "# lambda_matrix_target:\n",
        "#  [[1.  0.  0.3]\n",
        "#  [0.  5.  4. ]\n",
        "#  [0.1 3.  0. ]]\n",
        "# target_P1:\n",
        "#  [[0.7 0.  0.3]\n",
        "#  [0.1 0.5 0.4]\n",
        "#  [1.  0.  0. ]]\n",
        "# target_P2:\n",
        "#  [[0.1 0.6 0.3]\n",
        "#  [0.  1.  0. ]\n",
        "#  [0.1 0.5 0.4]]\n",
        "# Tensor P:\n",
        "#  [[[ 0.69999999  0.          0.30000001]\n",
        "#   [ 0.1         0.60000002  0.30000001]\n",
        "#   [ 0.19        0.54000002  0.27000001]]\n",
        "\n",
        "#  [[ 0.          1.          0.        ]\n",
        "#   [ 0.5        -1.5         2.        ]\n",
        "#   [ 3.         -2.          0.        ]]\n",
        "\n",
        "#  [[ 0.28000001  0.34999999  0.37      ]\n",
        "#   [ 0.1         0.5         0.40000001]\n",
        "#   [ 0.1         0.5         0.40000001]]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4zLcxfL900G0",
      "metadata": {
        "id": "4zLcxfL900G0"
      },
      "outputs": [],
      "source": [
        "P1:\n",
        " [[0.38066465 0.40785498 0.21148036]\n",
        " [0.10334347 0.51975684 0.3768997 ]\n",
        " [0.43529412 0.34705882 0.21764706]]\n",
        "P2:\n",
        " [[0.36363636 0.3553719  0.28099174]\n",
        " [0.37003058 0.48318043 0.14678899]\n",
        " [0.17741935 0.44193548 0.38064516]]\n",
        "lambda_matrix:\n",
        " [[0.37454012 0.95071431 0.73199394]\n",
        " [0.59865848 0.15601864 0.15599452]\n",
        " [0.05808361 0.86617615 0.60111501]]\n",
        "iteration: 0\n",
        "iteration: 1\n",
        "Iteration 2, Max change: 4.843866880999313\n",
        "Updated lambda_matrix:\n",
        " [[0.37451983 0.95069715 0.73197227]\n",
        " [0.59872107 0.15613312 0.15600621]\n",
        " [0.05807985 0.86621161 0.60101067]]\n",
        "Iteration 2, Max change: 0.5647058823529412\n",
        "Updated P1:\n",
        " [[0.38066465 0.40785498 0.21148036]\n",
        " [0.10334347 0.51975684 0.3768997 ]\n",
        " [0.43529412 0.34705882 0.21764706]]\n",
        "Iteration 2, Max change: 0.5168195718654434\n",
        "Updated P2:\n",
        " [[0.36363636 0.3553719  0.28099174]\n",
        " [0.37003058 0.48318043 0.14678899]\n",
        " [0.17741935 0.44193548 0.38064516]]\n",
        "Tensor P_hat:\n",
        " [[[0.3700138  0.37502786 0.25495835]\n",
        "  [0.20779352 0.45379263 0.33841385]\n",
        "  [0.36779824 0.35488908 0.27731269]]\n",
        "\n",
        " [[0.38014036 0.41156874 0.20829089]\n",
        "  [0.32839189 0.48889122 0.18271689]\n",
        "  [0.42656261 0.36527031 0.20816707]]\n",
        "\n",
        " [[0.32618928 0.4169895  0.25682122]\n",
        "  [0.16586306 0.4540761  0.38006085]\n",
        "  [0.33240484 0.3849136  0.28268156]]]\n",
        "-----------------------------------------\n",
        "empirical_cross_entropy: 2.833268352935607\n",
        "\n",
        "\n",
        "\n",
        "Iteration 10, Max change: 4.843939722396137\n",
        "Updated lambda_matrix:\n",
        " [[0.37450488 0.95071007 0.73197872]\n",
        " [0.5986896  0.15606028 0.15599884]\n",
        " [0.05807699 0.86621918 0.6010134 ]]\n",
        "Iteration 10, Max change: 0.6156376224993734\n",
        "Updated P1:\n",
        " [[0.29299369 0.61563762 0.09136869]\n",
        " [0.02315317 0.64957273 0.3272741 ]\n",
        " [0.84303833 0.10846669 0.04849498]]\n",
        "Iteration 10, Max change: 0.5002168101794461\n",
        "Updated P2:\n",
        " [[0.38695262 0.32137464 0.29167275]\n",
        " [0.13547244 0.49978319 0.36474437]\n",
        " [0.04817241 0.42995401 0.52187359]]\n",
        "Tensor P_hat:\n",
        " [[[0.35176454 0.43157756 0.2166579 ]\n",
        "  [0.16914967 0.51786342 0.31298691]\n",
        "  [0.4134407  0.30900959 0.27754971]]\n",
        "\n",
        " [[0.28522947 0.60992717 0.10484336]\n",
        "  [0.11794386 0.52315939 0.35889675]\n",
        "  [0.74837959 0.16081733 0.09080308]]\n",
        "\n",
        " [[0.22737637 0.56587046 0.20675317]\n",
        "  [0.04426943 0.46421427 0.49151629]\n",
        "  [0.52589748 0.23673582 0.2373667 ]]]\n",
        "-----------------------------------------\n",
        "empirical_cross_entropy: 2.555986311895934"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}